{
    "id": "wanzinyazar",
    "name": "Wanzin Yazar",
    "profile_pic": "https://static.licdn.com/aero-v1/sc/h/9c8pery4andzj6ohjkjp54ma2",
    "links": [
        "https://www.coursera.org/account/accomplishments/certificate/H9MJRE7SJLAM",
        "https://www.coursera.org/account/accomplishments/specialization/certificate/LMLL7UPCTXYQ",
        "https://learn.xnextcon.com/index/certificate/2020013109waya",
        "https://arxiv.org/pdf/2405.07135",
        "https://arxiv.org/pdf/2404.09336",
        "https://arxiv.org/pdf/2410.14570",
        "https://arxiv.org/pdf/2410.12119"
    ],
    "short_description": "AI researcher at d-Matrix and UC Berkeley alumnus specializing in LLM optimization. Published work on quantized LLMs and attention span acceleration for inference. Award-winning hackathon participant with expertise in cloud computing, deep learning, and healthcare AI applications.",
    "long_description": "Wanzin Yazar is a Staff ML Research Engineer at d-Matrix, a company focused on in-memory compute architecture for AI applications, and is an alumnus of the University of California, Berkeley, where they earned a Bachelor's degree in Applied Mathematics with a focus on AI. Their research focuses on optimizing large language models (LLMs) for more efficient inference and deployment.\n\nWanzin has published several significant papers in the field of AI optimization, including \"Post Training Quantization of Large Language Models with Microscaling Formats\" (2024), \"Self-Selected Attention Span for Accelerating Large Language Model Inference\" (2024), \"Understanding the difficulty of low-precision post-training quantization of large language models\" (2024), and \"Scaling laws for post-training quantized large language models\" (2024). Their work demonstrates expertise in model quantization and inference acceleration techniques, which are critical for deploying AI models in resource-constrained environments.\n\nBeyond LLM optimization, Wanzin has also contributed to research in healthcare AI applications, with publications like \"Through the Looking Glass: Unraveling the Stage-Shift of Acute Rejection in Renal Allografts\" (2022) and \"The self-upgrading mobile application for the automatic malaria detection\" (2020). At Nephrosant, Inc., they developed productionized machine learning models from large healthcare datasets, showing versatility in applying AI to solve real-world problems in the medical field.\n\nWanzin previously worked as a Machine Learning Researcher at UC Berkeley's Division of Computing, Data Science, and Society, focusing on deep learning-based recommender systems and student enrollment predictions using clustering practices and regression models. They also gained experience at Advancers.ai before joining d-Matrix.\n\nWanzin has an impressive track record in hackathons, winning the Grand Prize at AngelHack Silicon Valley 2019, Best Mobile Hack at SF HACK 2019, and the Best Asian Pacific Quest in the 2019 IBM Call For Code Global Challenge. These achievements highlight their ability to rapidly prototype and implement AI solutions.\n\nTheir professional certifications include Google Cloud Certified Data Engineer (issued January 2020), the Deep Learning Specialization from deeplearning.ai (issued January 2018), and Full Stack Machine Learning on AWS from AI Camp. These certifications demonstrate Wanzin's comprehensive understanding of cloud infrastructure, deep learning fundamentals, and practical implementation of machine learning systems.\n\nWanzin's research has been cited by other scholars in the field, with their papers on LLM optimization and healthcare applications receiving recognition in the academic community. Their expertise spans machine learning, pattern recognition, image processing, neural networks, computer vision, data mining, computational intelligence, statistical learning, and data analysis."
}