<!DOCTYPE HTML>
<!--
	Editorial by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <title>Projects | CAIS++</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="icon" href="/images/circle.png">
    <link rel="stylesheet" href="/assets/css/main.css"/>
    
    <link rel="stylesheet" href="/assets/css/projects.css"/>
    
    
    
    </script>
  </head>
  <body class="is-preload">
    <!-- Wrapper -->
    <div id="wrapper">
      <!-- Main -->
      <div id="main">
        <div class="inner">
          <!-- Banner -->
<section id="banner">
  <div class="content">
    <header>
      <h1>PROJECTS</h1>
    </header>
    <h4>WHAT WE'VE BEEN UP TO</h4>
    <div class="tabContainer">
      <a href="/projects/f24-s25" class="button">F24 - S25</a>
      <a href="/projects/f23-s24" class="active button">F23 - S24</a>
      <a href="/projects/f22-s23" class="button">F22 - S23</a>
      <a href="/projects/f21-s22" class="button">F21 - S22</a>
      <a href="/projects/f20-s21" class="button">F20 - S21</a>
      <a href="/projects/f19-s20" class="button">F19 - S20</a>
      <a href="/projects/f18-s19" class="button">F18 - S19</a>
      <a href="/projects/s18" class="button">S18</a>
    </div>
  </div>
</section>

<!-- Project List Builder -->




<!-- Section -->
<section>
  <header class="major">
    <h2>SPRING 2024</h2>
  </header>
  
  
  <article id=s24_robotics>
  
    <div class="image"><img src=/images/projects/robotics.png alt="" /></div>
    <div class="content">
      <h3>Gen-AI for Multi-Agent Deformable Object Manipulation</h3>
      <h5>
        
        Jonathan Ong&nbsp;<span class='accent'>*</span> ,
        
        Zitong Huang&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Rida Faraz</span> ,
        
        <span>Vijay Kumaravelrajan</span> ,
        
        <span>Siddharth Rudharaju</span> ,
        
        <span>Anisha Chitta</span>
        
        ,
        
        <span>Daniel Seita</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Robots hold great potential for automating tasks across various environments, yet their adoption is limited by their current capabilities, especially in complex scenarios like caregiving. To enhance automation and accessibility, this project works to advance robotic manipulation of objects that require multiple manipulators and deformable items such as bread and clothing. In this work, we integrate various robotic platforms, including ALOHA and Baxter, into the MuJoCo simulation environment using Robosuite, and set up bimanual configurations to explore different arm controllers. To robustly assess robotic performance, we identify specific tasks involving deformable objects and continuously refine our evaluation protocols.</p>
      <ul class="actions">
        
        
        <li><a href="https://www.canva.com/design/DAF1mxXM5j4/j3Ka5dkVim1q-4FYeW4xzA/edit?utm_content=DAF1mxXM5j4&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton" target="_blank" class="button">Poster</a></li>
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_brain>
  
    <div class="image"><img src=/images/projects/eegbrain.png alt="" /></div>
    <div class="content">
      <h3>Harmful Brain Activity Classification through EEG Spectrogram Data</h3>
      <h5>
        
        Jessica Fu&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Vayun Mathur</span> ,
        
        <span>Ryan Nene</span> ,
        
        <span>Brice Patchou</span>
        
        
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
      </h6>
      
      <p>Currently, electroencephalogram (EEG) monitoring heavily relies on manual analysis by specialized neurologists, leading to time-consuming and expensive procedures with potential errors. By applying deep learning techniques for EEG analysis, the Harmful Brain Activity Project strives to enhance the accuracy of electroencephalography pattern classification of harmful brain activities, such as seizures, to facilitate greater diagnoses and treatments for patients. By training convolutional neural networks (CNNs) in a specialized pipeline, we aim to create a highly effective model as a contribution towards AI applications in EEG analysis, thus positively impacting the preservation and advancement of human brain health.</p>
      <ul class="actions">
        
        
        
        <li><a href="https://docs.google.com/presentation/d/1xrhAqk4LP2bnbUJZrILtj2ECVtaHzhzzoxRfzY2mGbM/edit?usp=sharing" target="_blank" class="button">Slides</a></li>
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_citylearn>
  
    <div class="image"><img src=/images/projects/citylearn.png alt="" /></div>
    <div class="content">
      <h3>CityLearn: Reducing Greenhouse Gas Emissions by Improving Energy Distribution via Time Series Forecasting</h3>
      <h5>
        
        Sanjana Ilango&nbsp;<span class='accent'>*</span> ,
        
        Spencer Tran&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Vidur Mushran</span> ,
        
        <span>Andrew Choi</span> ,
        
        <span>Joanne Lee</span> ,
        
        <span>Jimena Arce</span>
        
        
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
      </h6>
      
      <p>Buildings account for 30% of greenhouse gas emissions, making energy efficiency a critical focus for sustainability efforts. Distributed energy resources, such as domestic hot water systems that store electricity and solar panels that generate it, play a key role in alleviating the strain on building electric grids. To optimize the management and allocation of these resources across multiple buildings, it is essential to develop accurate and reliable energy usage predictions. This project aims to create robust predictive models that will enhance energy efficiency and contribute to reducing the environmental impact of buildings.</p>
      <ul class="actions">
        
        
        <li><a href="https://www.canva.com/design/DAGCuq1cTtk/itS-dWmLqUJKZJanR3W9tw/edit?utm_content=DAGCuq1cTtk&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton" target="_blank" class="button">Poster</a></li>
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_hatespeech>
  
    <div class="image"><img src=/images/projects/hatespeech.png alt="" /></div>
    <div class="content">
      <h3>Comparing Encoder-Decoder Architectures for Multimodal Hate Speech Detection in Hateful Memes Dataset</h3>
      <h5>
        
        Nathan Johnson&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Maohe (Mo) Jiang</span> ,
        
        <span>Jonathan Aydin</span> ,
        
        <span>Catherine Lu</span> ,
        
        <span>Darius Mahjoob</span> ,
        
        <span>Catherine He</span>
        
        
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
      </h6>
      
      <p>Multimodal hate speech detection presents additional challenges beyond unimodal detection, as subtle forms of hate speech often surface only when both text and images are analyzed together. This project examines the effectiveness and limitations of various pre-trained multimodal encoders in classifying content as hate speech. By comparing these models, we aim to identify the most viable approaches for accurately detecting hate speech in complex, multimodal contexts.</p>
      <ul class="actions">
        
        
        <li><a href="https://www.canva.com/design/DAGCu8a4mJ8/8MfftUbnzWRh4RzkQmyR5w/edit?utm_content=DAGCu8a4mJ8&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton" target="_blank" class="button">Poster</a></li>
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_reranking>
  
    <div class="image"><img src=/images/projects/reranking.png alt="" /></div>
    <div class="content">
      <h3>Post-generation ASR Hypothesis Reranking Utilizing Visual Contexts (1.0)</h3>
      <h5>
        
        Youqi Huang&nbsp;<span class='accent'>*</span> ,
        
        Aryan Trehan&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Marcus Au</span> ,
        
        <span>Stan Loosmore</span> ,
        
        <span>Tommy Shu</span> ,
        
        <span>Yirui Song</span>
        
        
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
      </h6>
      
      <p>The Automatic Speech Recognition (ASR) pipeline proposed in "Multimodal Speech Recognition for Language-Guided Embodied Agents" (Chang et al.) processes both unimodal (audio-only) and multimodal (audiovisual) data to generate multiple ranked hypotheses based on a given ground truth statement. However, the model often fails to rank the hypothesis with the lowest Word Error Rate (WER) as the top choice. To address this issue, we propose a multimodal reranking pipeline that leverages the same visual cues used in the ASR process.</p>
      <ul class="actions">
        
        
        <li><a href="https://www.canva.com/design/DAGChMQRHWE/mk30RZopkH3Sk0deJQq0ag/edit?utm_content=DAGChMQRHWE&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton" target="_blank" class="button">Poster</a></li>
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_llmbias>
  
    <div class="image"><img src=/images/projects/llmbias.png alt="" /></div>
    <div class="content">
      <h3>Gender Workplace Bias in Large Language Models</h3>
      <h5>
        
        Rachita Jain&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Jessica Luna</span> ,
        
        <span>Kailin Xia</span> ,
        
        <span>Arjun Bedi</span> ,
        
        <span>Malina Freeman</span>
        
        
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
      </h6>
      
      <p>As large language models (LLMs) like ChatGPT gain prominence, addressing the biases embedded within these models is of paramount importance, particularly regarding gender and gender roles in the workplace. Historically, certain occupations have been linked with specific genders, resulting in significant imbalances across various sectors. To bridge this gender gap, it is essential that AI technologies do not perpetuate these biases. This project leverages LLaMA, a widely adopted open-source LLM, and employs prompt engineering techniques to investigate methods for reducing workplace gender bias in model responses. The objective is to ensure that LLMs exhibit gender neutrality, particularly in contexts characterized by ambiguity or uncertainty.</p>
      <ul class="actions">
        
        
        <li><a href="https://docs.google.com/presentation/d/1g1ZWQciXAJ2kxgY8lZbXCsxTLJJ46rKMTEo_QNJwA-0/edit?usp=sharing" target="_blank" class="button">Poster</a></li>
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_turbulence>
  
    <div class="image"><img src=/images/projects/turbulence.png alt="" /></div>
    <div class="content">
      <h3>Navigating Climate-Induced Turbulence: Optimizing Aviation Emissions Through Neural Network-Based Turbulence Detection (2.0)</h3>
      <h5>
        
        Jayne Bottarini&nbsp;<span class='accent'>*</span> ,
        
        Jaiv Doshi&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Pratyush Jaishanker</span> ,
        
        <span>Naina Panjwani</span> ,
        
        <span>Sanya Verma</span> ,
        
        <span>Lauren Sun</span> ,
        
        <span>Jay Campanell</span>
        
        ,
        
        <span>Sam Silva</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Aviation currently contributes to over 3% of global carbon dioxide emissions and will only contribute more as global travel levels increase [1], which is why implementing optimal flight paths based on emissions is essential. As the Earth warms, however, atmospheric wind patterns change unpredictably, inducing a cycle of more turbulent air, suboptimal flight paths, and increased emissions of carbon dioxide and other pollutants. We aim to contribute to breaking this cycle by employing generative and spatial machine learning techniques to downscale wind patterns in order to improve finer-scale wind speed predictions and inform emissions-based optimization techniques in flight paths.</p>
      <ul class="actions">
        
        
        
        <li><a href="https://docs.google.com/presentation/d/1vQKWCxAcEtK2Ltn9a4QaxLIJicI_SVbZcfC9B6-9NWo/edit?usp=sharing" target="_blank" class="button">Slides</a></li>
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_ear>
  
    <div class="image"><img src=/images/projects/ear.png alt="" /></div>
    <div class="content">
      <h3>Computer Vision and Machine Learning on Optical Coherence Tomography for Middle Ear Pathology Detection (2.0)</h3>
      <h5>
        
        Claude Yoo&nbsp;<span class='accent'>*</span> ,
        
        Lucia Zhang&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Seena Pourzand</span> ,
        
        <span>Irika Katiyar</span> ,
        
        <span>Will Dolan</span> ,
        
        <span>Matthew Rodriguez</span> ,
        
        <span>Sana Jayaswal</span>
        
        ,
        
        <span>Brian Applegate</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Current diagnostic methods for middle ear diseases in otology are primarily qualitative and limited to examining only the surface of the tympanic membrane (TM). Optical Coherence Tomography (OCT) offers a non-invasive, quantitative imaging technique that enables three-dimensional reconstruction of the TM and middle ear, providing more detailed information than traditional methods. However, manually interpreting OCT scans can be time-consuming and challenging, and while OCT-based disease detection models are well-established in retinal imaging and ophthalmology, their application in otology remains relatively unexplored. This project focuses on creating a multi-classification machine learning model capable of identifying conditions such as retraction pockets, perforations, and cholesteatomas, and distinguishing them from healthy ear scans.</p>
      <ul class="actions">
        
        
        
        <li><a href="https://docs.google.com/presentation/d/1OOsHgjhT2zlJJDHK_HXvYxj3KLoxKAwtTfmiBsNConE/edit?usp=sharing" target="_blank" class="button">Slides</a></li>
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_graph>
  
    <div class="image"><img src=/images/projects/graph.png alt="" /></div>
    <div class="content">
      <h3>Performance-based Feature Sampling for Reducing Bias in Image Recognition Models (2.0)</h3>
      <h5>
        
        Aarav Monga&nbsp;<span class='accent'>*</span> ,
        
        Sonia Zhang&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Advik Unni</span> ,
        
        <span>Shahzeb Lakhani</span> ,
        
        <span>Rajakrishnan Somou</span>
        
        ,
        
        <span>Antonio Ortega</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>The presence of unseen bias in machine learning models remains a significant barrier to achieving trusted AI. While class imbalance is often addressed through training set sampling methods, this project targets a more nuanced challenge: bias within specific groups of a class. Such biases can create harmful, spurious correlations—like associating certain demographics with particular roles—that undermine the accuracy and fairness of representation learning. Addressing these issues involves a two-step process: first, identifying and labeling the bias groups, and second, adaptively sampling from these groups during training. This approach aims to correct the hidden biases that complicate model training and ultimately enhance the reliability of AI systems.</p>
      <ul class="actions">
        
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=s24_linguistics>
  
    <div class="image"><img src=/images/projects/conversation.png alt="" /></div>
    <div class="content">
      <h3>Indigenous Language Translation with Sparse Data (4.0)</h3>
      <h5>
        
        Aryan Gulati&nbsp;<span class='accent'>*</span> ,
        
        Leslie Moreno&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Abhinav Gupta</span> ,
        
        <span>Aditya Kumar</span>
        
        ,
        
        <span>Jonathan May</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Imperialism has led to a loss of many indigenous cultures and with this, their languages. Based on the NeurIPS 2022 Competition “Second AmericasNLP Competition: Speech-to-Text Translation for Indigenous Languages of the Americas,” this project aims to use machine translation (MT) and automatic speech recognition (ASR) approaches to develop a translator for endangered or extinct indigenous languages. This will involve finding and/or building an appropriately sized corpus and using this to train MT and ASR models due to the sparsity of data on these indigenous languages.</p>
      <ul class="actions">
        
        
        
        <li><a href="https://docs.google.com/presentation/d/1QNkHEsl4WOpEN-O8JiayVwYKPWM5FUJXPyxL11UG2YE/edit?usp=sharing" target="_blank" class="button">Slides</a></li>
        
        
        
      </ul>
    </div>
  </article>
  
</section>



<!-- Section -->
<section>
  <header class="major">
    <h2>FALL 2023</h2>
  </header>
  
  
  <article id=f23_turbulence>
  
    <div class="image"><img src=/images/projects/turbulence.png alt="" /></div>
    <div class="content">
      <h3>Navigating Climate-Induced Turbulence: Optimizing Aviation Emissions Through Neural Network-Based Turbulence Detection (1.0)</h3>
      <h5>
        
        Jayne Bottarini&nbsp;<span class='accent'>*</span> ,
        
        Jaiv Doshi&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Pratyush Jaishanker</span> ,
        
        <span>Naina Panjwani</span> ,
        
        <span>Sanya Verma</span>
        
        ,
        
        <span>Sam Silva</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Aviation currently contributes to over 3% of global carbon dioxide emissions and will only contribute more as global travel levels increase [1], which is why implementing optimal flight paths based on emissions is essential. As the Earth warms, however, atmospheric wind patterns change unpredictably, inducing a cycle of more turbulent air, suboptimal flight paths, and increased emissions of carbon dioxide and other pollutants. We aim to contribute to breaking this cycle by employing generative and spatial machine learning techniques to downscale wind patterns in order to improve finer-scale wind speed predictions and inform emissions-based optimization techniques in flight paths.</p>
      <ul class="actions">
        
        
        
        <li><a href="https://docs.google.com/presentation/d/1vQKWCxAcEtK2Ltn9a4QaxLIJicI_SVbZcfC9B6-9NWo/edit?usp=sharing" target="_blank" class="button">Slides</a></li>
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=f23_ear>
  
    <div class="image"><img src=/images/projects/ear.png alt="" /></div>
    <div class="content">
      <h3>Computer Vision and Machine Learning on Optical Coherence Tomography for Middle Ear Pathology Detection (1.0)</h3>
      <h5>
        
        Claude Yoo&nbsp;<span class='accent'>*</span> ,
        
        Lucia Zhang&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Sana Jayaswal</span> ,
        
        <span>Seena Pourzand</span> ,
        
        <span>Irika Katiyar</span> ,
        
        <span>Will Dolan</span>
        
        ,
        
        <span>Brian Applegate</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Current diagnostic methods for middle ear diseases in otology are primarily qualitative and limited to examining only the surface of the tympanic membrane (TM). Optical Coherence Tomography (OCT) offers a non-invasive, quantitative imaging technique that enables three-dimensional reconstruction of the TM and middle ear, providing more detailed information than traditional methods. However, manually interpreting OCT scans can be time-consuming and challenging, and while OCT-based disease detection models are well-established in retinal imaging and ophthalmology, their application in otology remains relatively unexplored. This project focuses on creating a multi-classification machine learning model capable of identifying conditions such as retraction pockets, perforations, and cholesteatomas, and distinguishing them from healthy ear scans.</p>
      <ul class="actions">
        
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=f23_graph>
  
    <div class="image"><img src=/images/projects/graph.png alt="" /></div>
    <div class="content">
      <h3>Performance-based Feature Sampling for Reducing Bias in Image Recognition Models (1.0)</h3>
      <h5>
        
        Aarav Monga&nbsp;<span class='accent'>*</span> ,
        
        Sonia Zhang&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Advik Unni</span> ,
        
        <span>Shahzeb Lakhani</span> ,
        
        <span>Rajakrishnan Somou</span>
        
        ,
        
        <span>Antonio Ortega</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>The presence of unseen bias in machine learning models remains a significant barrier to achieving trusted AI. While class imbalance is often addressed through training set sampling methods, this project targets a more nuanced challenge: bias within specific groups of a class. Such biases can create harmful, spurious correlations—like associating certain demographics with particular roles—that undermine the accuracy and fairness of representation learning. Addressing these issues involves a two-step process: first, identifying and labeling the bias groups, and second, adaptively sampling from these groups during training. This approach aims to correct the hidden biases that complicate model training and ultimately enhance the reliability of AI systems.</p>
      <ul class="actions">
        
        
        
        
        
      </ul>
    </div>
  </article>
  
  
  <article id=f23_linguistics>
  
    <div class="image"><img src=/images/projects/conversation.png alt="" /></div>
    <div class="content">
      <h3>Indigenous Language Translation with Sparse Data (3.0)</h3>
      <h5>
        
        Aryan Gulati&nbsp;<span class='accent'>*</span> ,
        
        Leslie Moreno&nbsp;<span class='accent'>*</span>
        
        ,
        
        <span>Abhinav Gupta</span> ,
        
        <span>Aditya Kumar</span>
        
        ,
        
        <span>Jonathan May</span>&nbsp;<span class='accent-yellow'>*</span>&nbsp;
        
      </h5>
      
      <h6>
        
        <span class='accent'>*</span>&nbsp;Project Lead&nbsp;&nbsp;&nbsp;
        
        
        <span class='accent-yellow'>*</span>&nbsp;Project Advisor
        
      </h6>
      
      <p>Imperialism has led to a loss of many indigenous cultures and with this, their languages. Based on the NeurIPS 2022 Competition “Second AmericasNLP Competition: Speech-to-Text Translation for Indigenous Languages of the Americas,” this project aims to use machine translation (MT) and automatic speech recognition (ASR) approaches to develop a translator for endangered or extinct indigenous languages. This will involve finding and/or building an appropriately sized corpus and using this to train MT and ASR models due to the sparsity of data on these indigenous languages.</p>
      <ul class="actions">
        
        
        
        <li><a href="https://docs.google.com/presentation/d/1iiA5D7VuvZC4WiS8DRJBSVI5pz0Prw3O6VUdUzHv5Sg/edit?usp=sharing" target="_blank" class="button">Slides</a></li>
        
        
        
      </ul>
    </div>
  </article>
  
</section>
























<section class="pageNav">
  <a href="" class="button"
     
     style="visibility: hidden;"
     
  ><i class="fas fa-angle-left"></i> Previous</a>
  <a href="/projects/f22-s23" class="button"
     
  >Next <i class="fas fa-angle-right"></i></a>
</section>
        </div>
      </div>

      <!-- Sidebar -->
      <div id="sidebar">
  <div class="inner">
    <!-- Menu -->
    <nav id="menu">
      <header class="major">
        <a href="/">
          <img src="/images/whitecircle.png" style="height: 100%; object-fit: contain; border: 0" align="left">
          <h2>CAIS++</h2>
        </a>
      </header>
      <ul>
        <li><a href="/index">About</a></li>
        <li><a href="/projects">Projects</a></li>
        <li>
          <span class="opener">People</span>
          <ul>
            <li><a href="/people">Current Members</a></li>
            <li><a href="/people/alumni">Alumni</a></li>
            <li><a href="/curriculum/seminar">Invited Speakers</a></li>
          </ul>
        </li>
        <li>
          <span class="opener">Curriculum</span>
          <ul>
            <li><a href="/curriculum">Overview</a></li>
            <li><a href="/curriculum/intro">[1] Introduction to AI</a></li>
            <li>
              <span class="opener curriculum-opener">[2] Classical Machine Learning</span>
              <ul>
                <li><a href="/curriculum/classical/knn">[2.1] k-Nearest Neighbors</a></li>
                <li><a href="/curriculum/classical/linear-regression">[2.2] Linear Regression</a></li>
                <li><a href="/curriculum/classical/logistic-regression">[2.3] Logistic Regression</a></li>
                <li><a href="/curriculum/classical/support-vector-machines">[2.4] Support Vector Machines</a></li>
                <li><a href="/curriculum/classical/decision-trees">[2.5] Decision Trees</a></li>
                <li><a href="/curriculum/classical/naive-bayes">[2.6] Naive Bayes</a></li>
              </ul>
            </li>
            <li>
              <span class="opener curriculum-opener">[3] Neural Networks</span>
              <ul>
                <li><a href="/curriculum/neural-networks/architecture">[3.1] Architecture</a></li>
                <li><a href="/curriculum/neural-networks/training">[3.2] Training</a></li>
                <li><a href="/curriculum/neural-networks/optimization">[3.3] Optimization</a></li>
              </ul>
            </li>
            <li>
              <span class="opener curriculum-opener">[4] Neural Network Flavors</span>
              <ul>
                <li><a href="/curriculum/neural-network-flavors/convolutional-neural-networks">[4.1] Convolutional Neural Networks</a></li>
                <li><a href="/curriculum/neural-network-flavors/recurrent-neural-networks">[4.2] Recurrent Neural Networks</a></li>
                <li><a href="/curriculum/neural-network-flavors/transformers">[4.3] Attention & Transformers</a></li>
                <li><a href="/curriculum/neural-network-flavors/generative-adversarial-networks">[4.4] Generative Adversarial Networks</a></li>
              </ul>
            </li>
            <span class="opener curriculum-opener">[5] Special Topics</span>
            <ul>
              <li><a href="/curriculum/special-topics/transfer-learning">[5.1] Transfer Learning</a></li>
              <li><a href="/curriculum/special-topics/reinforcement-learning">[5.2] Reinforcement Learning</a></li>
            </ul>
            <li><a href="/curriculum/reading-list">Reading List</a></li>
            <li><a href="/curriculum/seminar">Speaker Seminar</a></li>
          </ul>
        </li>
        <li><a href="/#applications">Apply</a></li>
        <li><a href="/contact">Contact Us</a></li>
      </ul>
    </nav>

    <!-- Section -->
    <section id="contact-section">
<!--      <header class="major">-->
<!--        <h2>Get in touch</h2>-->
<!--      </header>-->
<!--      <h3 class="nomargin">For Clients/Professors</h3>-->
<!--      <p>Since our inception, our members have been striving to apply their skills in Artificial Intelligence to help solve the real problems in communities around us. Our student teams work with nonprofits, researchers, and community leaders on semester-long projects to ensure that new advancements in AI are used for social good.  At the moment, our projects are primarily centered around machine learning, a subfield of Artificial Intelligence that focuses on leveraging large amounts of data to build statistical models that are able to generate insights and predictions that traditional software can’t.</p>-->
<!--      <a href="/contact/clients" class="button big">More Info</a>-->

<!--      <h3>For Sponsors</h3>-->
<!--      <p>The support of alumni, corporations, foundations, and individuals makes a real difference to the organization and our ability to make an impact.</p>-->
<!--      <a href="/contact/sponsors" class="button big">More Info</a>-->

<!--      <h3>For Prospective Members</h3>-->
<!--      <p>We'd love to have you join our community! Please find our information on applying <a href='/#applications'>here</a>.</p>-->

      <h3>Contact Information</h3>

      <ul class="contact">
<!--        <li class="icon solid fa-envelope"><a href="mailto:caisplus@usc.edu">caisplus@usc.edu</a></li>-->
        <li>
          <a class="fas fa-envelope" href="mailto:caisplus@usc.edu" target="_blank"></a>
          <a class="fab fa-twitter" href="https://twitter.com/CAISplusplus" target="_blank"></a>
          <a class="fab fa-instagram" href="https://www.instagram.com/caisplusplus/" target="_blank"></a>
          <a class="fab fa-github" href="https://github.com/usc-caisplusplus" target="_blank"></a>
        </li>
      </ul>
    </section>

    <section id="sponsors">
      <header class="major">
        <h2>Our Sponsors</h2>
      </header>
      <div class="content">
        <img src="/images/sponsorship/sponsors/apple.png">
        <img src="/images/sponsorship/sponsors/boeing.png" style="height: 2.5em;">
        <img src="/images/sponsorship/sponsors/chevron.png">
        <img src="/images/sponsorship/sponsors/wintec.png" style="height: 1.8em;">
        <img src="/images/sponsorship/sponsors/twosixlabs.png" style="height: 2.5em;">
        <img src="/images/sponsorship/sponsors/nvidia.png">
      </div>
    </section>

    <!-- Footer -->
    <footer id="footer">
      <p class="copyright">&copy; CAIS++ Artificial Intelligence for social good. <br>
        Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a>.</p>
    </footer>
  </div>
</div>
    </div>

    <!-- Scripts -->
    <script src="/assets/js/jquery.min.js"></script>
    <script src="/assets/js/browser.min.js"></script>
    <script src="/assets/js/breakpoints.min.js"></script>
    <script src="/assets/js/util.js"></script>
    <script src="/assets/js/main.js"></script>
    
  </body>
</html>