      start desktop header
          We gratefully acknowledge support from the Simons Foundation,
            [member institutions](https://info.arxiv.org/about/ourmembers.html)
            , and all contributors.
          [Donate](https://info.arxiv.org/about/donate.html)
          >
          [cs](/list/cs/recent)
          arXiv:2410.14570
                  [Help](https://info.arxiv.org/help)
                  |
                  [Advanced Search](https://arxiv.org/search/advanced)
                    All fields
                    Title
                    Author
                    Abstract
                    Comments
                    Journal reference
                    ACM classification
                    MSC classification
                    Report number
                    arXiv identifier
                    DOI
                    ORCID
                    arXiv author ID
                    Help pages
                    Full text
              Search
      /end desktop header
                open search
                  GO
                open navigation menu
                quick links
                    [Login](https://arxiv.org/login)
                    [Help Pages](https://info.arxiv.org/help)
                    [About](https://info.arxiv.org/about)
      /end mobile-header
        rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2410.14570"
        dc:identifier="/abs/2410.14570"
        dc:title="Understanding the difficulty of low-precision post-training quantization of large language models"
        trackback:ping="/trackback/2410.14570" />
    </rdf:RDF>
              Computer Science > Machine Learning
              (cs)
                [Submitted on 18 Oct 2024]
                  Title:
                  Understanding the difficulty of low-precision post-training quantization of large language models
                  Authors:
                  [Zifei Xu](https://arxiv.org/search/cs?searchtype=author&query=Xu,+Z)
                  ,
                  [Sayeh Sharify](https://arxiv.org/search/cs?searchtype=author&query=Sharify,+S)
                  [Wanzin Yazar](https://arxiv.org/search/cs?searchtype=author&query=Yazar,+W)
                  [Tristan Webb](https://arxiv.org/search/cs?searchtype=author&query=Webb,+T)
                  [Xin Wang](https://arxiv.org/search/cs?searchtype=author&query=Wang,+X)
                View a PDF of the paper titled Understanding the difficulty of low-precision post-training quantization of large language models, by Zifei Xu and 4 other authors
                [View PDF](/pdf/2410.14570)
                [HTML (experimental)](https://arxiv.org/html/2410.14570v1)
                  Abstract:
                  Large language models of high parameter counts are computationally expensive, yet can be made much more efficient by compressing their weights to very low numerical precision. This can be achieved either through post-training quantization by minimizing local, layer-wise quantization errors, or through quantization-aware fine-tuning by minimizing the global loss function. In this study, we discovered that, under the same data constraint, the former approach nearly always fared worse than the latter, a phenomenon particularly prominent when the numerical precision is very low. We further showed that this difficulty of post-training quantization arose from stark misalignment between optimization of the local and global objective functions. Our findings explains limited utility in minimization of local quantization error and the importance of direct quantization-aware fine-tuning, in the regime of large models at very low precision.
                CONTEXT
                        Subjects:
                          Machine Learning (cs.LG)
                        Cite as:
                            [cs.LG]
                        (or
                            [arXiv:2410.14570v1](https://arxiv.org/abs/2410.14570v1)
                          for this version)
                          [https://doi.org/10.48550/arXiv.2410.14570](https://doi.org/10.48550/arXiv.2410.14570)
                              Focus to learn more
                            tooltip description
              Submission history
              From: Zifei Xu [
              [view email](/show-email/cd2b9eea/2410.14570)
              ]
              [v1]
              Fri, 18 Oct 2024 16:16:52 UTC (3,629 KB)
          end leftcolumn
              Full-text links:
              Access Paper:
                  [TeX Source](/src/2410.14570)
                  [Other Formats](/format/2410.14570)
                  view license
            end full-text
            Current browse context:
              cs.LG
                  [< prev](/prevnext?id=2410.14570&function=prev&context=cs.LG)
                  [next >](/prevnext?id=2410.14570&function=next&context=cs.LG)
                [new](/list/cs.LG/new)
                [recent](/list/cs.LG/recent)
                [2024-10](/list/cs.LG/2024-10)
              Change to browse by:
              References & Citations
                  [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.14570)
                  [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.14570)
                  [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.14570)
              export BibTeX citation
              Loading...
                  BibTeX formatted citation
                  ×
                  loading...
                  Data provided by:
                Bookmark
          end extra-services
          LABS AREA
              Bibliographic Tools
                Bibliographic and Citation Tools
                        Bibliographic Explorer Toggle
                      Bibliographic Explorer
                      (
                        [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer)
                        )
                        Connected Papers Toggle
                      Connected Papers
                        [What is Connected Papers?](https://www.connectedpapers.com/about)
                        Litmaps Toggle
                      Litmaps
                        [What is Litmaps?](https://www.litmaps.co/)
                        scite.ai Toggle
                      scite Smart Citations
                        [What are Smart Citations?](https://www.scite.ai/)
              Code, Data, Media
                Code, Data and Media Associated with this Article
                        alphaXiv Toggle
                      alphaXiv
                        [What is alphaXiv?](https://alphaxiv.org/)
                        Links to Code Toggle
                      CatalyzeX Code Finder for Papers
                        [What is CatalyzeX?](https://www.catalyzex.com)
                        DagsHub Toggle
                      DagsHub
                        [What is DagsHub?](https://dagshub.com/)
                        GotitPub Toggle
                      Gotit.pub
                        [What is GotitPub?](http://gotit.pub/faq)
                        Huggingface Toggle
                      Hugging Face
                        [What is Huggingface?](https://huggingface.co/huggingface)
                      Papers with Code
                        [What is Papers with Code?](https://paperswithcode.com/)
                        ScienceCast Toggle
                      ScienceCast
                        [What is ScienceCast?](https://sciencecast.org/welcome)
              Demos
                        Replicate Toggle
                      Replicate
                        [What is Replicate?](https://replicate.com/docs/arxiv/about)
                        Spaces Toggle
                      Hugging Face Spaces
                        [What is Spaces?](https://huggingface.co/docs/hub/spaces)
                      TXYZ.AI
                        [What is TXYZ.AI?](https://txyz.ai)
              Related Papers
                Recommenders and Search Tools
                        Link to Influence Flower
                      Influence Flower
                        [What are Influence Flowers?](https://influencemap.cmlab.dev/)
                        Core recommender toggle
                      CORE Recommender
                        [What is CORE?](https://core.ac.uk/services/recommender)
                        IArxiv recommender toggle
                      IArxiv Recommender
                        [What is IArxiv?](https://iarxiv.org/about)
              About arXivLabs
                    arXivLabs: experimental projects with community collaborators
                    arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
                    Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
                    Have an idea for a project that will add value for arXiv's community?
                        Learn more about arXivLabs
                      .
          END LABS AREA
            [Which authors of this paper are endorsers?](/auth/show-endorsers/2410.14570)
            [What is MathJax?](https://info.arxiv.org/help/mathjax.html)
        Macro-Column 1
                    contact arXiv
                    Click here to contact arXiv
                  [Contact](https://info.arxiv.org/help/contact.html)
                    subscribe to arXiv mailings
                    Click here to subscribe
                  [Subscribe](https://info.arxiv.org/help/subscribe)
        End Macro-Column 1
        Macro-Column 2
                  [Copyright](https://info.arxiv.org/help/license/index.html)
                  [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)
                  [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
                    [arXiv Operational Status](https://status.arxiv.org)
                    or
        end MetaColumn 2
        End Macro-Column 2