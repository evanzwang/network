{
    "id": "radostin-cholakov-bb4422146",
    "name": "Radostin Cholakov",
    "profile_pic": "https://media.licdn.com/dms/image/v2/D4D03AQHEJPyHH2j69w/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1683628003889?e=2147483647&v=beta&t=jK1Tv5KX4luuRipBWQYTtCZ6JDhvcQDN37tk-f9BVJQ",
    "links": [
        "https://github.com/RSG-Group/RSG-Chess-API",
        "https://anygoalanytime.com/",
        "http://smileforafrica.eu",
        "https://github.com/radi-cho/",
        "https://arxiv.org/abs/2208.07097",
        "https://aclanthology.org/2024.findings-emnlp.724/"
    ],
    "short_description": "Award-winning AI researcher, Stanford University undergraduate in Computer Science, and Forbes 30 Under 30 honoree specializing in natural language processing, quantization of large language models, and deep learning architectures for tabular data. Published author with significant contributions to efficient matrix multiplications for LLMs and task-oriented dialogue systems.",
    "long_description": "Radostin Cholakov is a distinguished young AI researcher currently pursuing undergraduate studies in Computer Science at Stanford University. His research focuses on innovative approaches to deep learning architectures, particularly for tabular data, dialogue systems, and large language model optimization.\n\nAs the author of \"Distributional Quantization of Large Language Models,\" Radostin has developed novel methods for efficiently storing LLMs in 4-bit precision while maintaining performance comparable to full-precision models. His recent work on \"Fast Matrix Multiplications for Lookup Table-Quantized LLMs\" (2024) with collaborators from MIT and CMU introduces FLUTE, a flexible lookup table engine that can be 2-4x faster than existing GEMM kernels for LLM inference.\n\nRadostin's contributions to task-oriented dialogue systems include developing models with auxiliary tasks for response selection, addressing limitations in text generation abilities of pre-trained language models. His paper \"Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task\" (2022) achieved state-of-the-art results on the MultiWOZ 2.1 dataset while using significantly fewer parameters than baseline models.\n\nHis work on the GatedTabTransformer has enhanced deep learning architectures for tabular modeling, showing significant improvements over existing solutions. Additionally, his research on \"Transformers predicting the future\" (2021) applies attention mechanisms to next-frame and time series forecasting, garnering significant citations in the field.\n\nRadostin is also the creator of \"AzBuki.ML,\" a machine learning platform for natural language processing specifically designed for Slavic languages.\n\nHis exceptional work has earned him numerous prestigious awards, including an Honorable Mention from AAAI and First Award by Mawhiba at Regeneron ISEF'23, Second Award at the European Contest For Young Scientists 2022, and Second award from ACM at Regeneron ISEF'22. In 2021, he was recognized in Forbes' \"30 under 30\" list and received the Presidential Award \"John Atanasoff\" for exceptional results in National IT Competitions.\n\nBeyond his research, Radostin is actively involved in volunteer work with organizations like Google Developer Experts, Smile for Africa Foundation (where he serves as a Full-Stack Software Engineer), and Google Developers Group Sofia. His academic collaborations include work with prominent researchers from MIT, including Yoon Kim, Han Guo, and Jonathan Ragan-Kelley, as well as Eric Xing from CMU.\n\nAs the founder of RSG, Radostin continues to push boundaries in AI research while contributing to open-source projects and humanitarian causes."
}