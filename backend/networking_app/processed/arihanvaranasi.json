{
    "id": "arihanvaranasi",
    "name": "Arihan V.",
    "profile_pic": "https://media.licdn.com/dms/image/v2/D5603AQH_KCQYYFwP8Q/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1696996010871?e=2147483647&v=beta&t=Fcy_XDLHudLk3e49aqkZBaW2febpWQYkU38vuu9jo-c",
    "links": [
        "https://arihanv.github.io/ReadSpeak/",
        "https://untcourseseeker.streamlit.app/",
        "https://github.com/arihanv",
        "https://twitter.com/ar1hanv",
        "https://www.linkedin.com/in/arihanvaranasi/",
        "https://arihanv.com/"
    ],
    "short_description": "Stanford CS student and accomplished ML researcher specializing in computer vision and LLM applications. With multiple hackathon wins and research experience at UNT's Computer Vision and Intelligent Systems Laboratory, Arihan has developed innovative solutions ranging from cropland optimization using neural networks to sign language interpretation systems.",
    "long_description": "Arihan V. is a promising talent in the field of machine learning and AI, currently pursuing a Computer Science degree at Stanford University (Class of 2028). His technical journey began early, with exceptional academic performance at the Texas Academy of Mathematics and Science and Coppell High School.\n\nAs an Undergraduate Researcher at the University of North Texas Computer Vision and Intelligent Systems Laboratory, Arihan has worked on sophisticated projects funded by NASA, USGS, and Physmodo Inc. He also contributed to research at UNT's Functional Glass Materials and Modeling (FGMM) Laboratory as a TAMS student. His research contributions include developing bi-stream convolutional neural networks and deep learning autoencoders to identify cropland levees and optimize farming structures, as well as creating a novel human tracking method using point clouds and depth videos to improve pose estimation accuracy.\n\nArihan has demonstrated remarkable success in competitive environments, winning multiple hackathons with innovative AI solutions. His projects showcase versatility across different domains:\n- Shush: A WhisperV3 model with Flash Attention v2 deployed on Modal via a NextJS app (197 GitHub stars)\n- CoWorkr: An AI-powered team knowledge bot\n- Koios GPT: A web application combining AI algorithms and LLMs to process research articles and PDFs (1st Place at HackLah! and Code4Good)\n- Sign Ease: An app utilizing machine learning to interpret sign language gestures (Winner at MLH's Hack Your Portfolio)\n- Transcribi: A web app leveraging OpenAI GPT, Whisper, and Langchain for audio transcription and analysis (1st Place Education Hack at PantherHack)\n- SightSense: A wearable device using object detection and text-to-speech to aid visually impaired individuals (1st Place Generative AI at Treasure Hacks)\n- ReadSpeak: A comprehensive app for improving reading and pronunciation skills (1st Place at LaunchHacks II)\n- PromptGPT: A tool that generates efficient prompts using a custom, finetuned, opensource Dolly v2 model\n- Zenset: An expressive writing and journaling app with rewards\n\nHis technical expertise spans Python, React, Next.js, TensorFlow, PyTorch, convolutional neural networks, machine learning, statistical analysis, and LLM applications. Arihan has also received recognition for his academic excellence, including the UNT President's List for a perfect 4.0 GPA and the TAMS Summer Scholarship for continued research work.\n\nAt Stanford, Arihan is part of the IRIS lab community, which focuses on robotics and machine learning research. He's also been selected as a National Security Innovation Scholar at the Gordian Knot Center for National Security Innovation.\n\nBeyond his technical accomplishments, Arihan has demonstrated business acumen as a DECA Collegiate Business Ethics State Finalist and ICDC Qualifier. His multilingual capabilities include proficiency in English (native), Hindi, Telugu, and Spanish, reflecting his diverse skill set and global perspective."
}