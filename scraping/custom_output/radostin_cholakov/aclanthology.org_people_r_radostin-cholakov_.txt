        ACL Anthology
            [News](/posts/)
              (current)
            [FAQ](/faq/)
            [Corrections](/info/corrections/)
            [Submissions](/info/contrib/)
        Radostin
        Cholakov
          2024
              [pdf](https://aclanthology.org/2024.findings-emnlp.724.pdf)
              [bib](/2024.findings-emnlp.724.bib)
                [Fast Matrix Multiplications for Lookup Table-Quantized](/2024.findings-emnlp.724/)
                  LLM
                  s
              [Han Guo](/people/h/han-guo/)
              |
              [William Brandon](/people/w/william-brandon/)
              [Radostin Cholakov](/people/r/radostin-cholakov/)
              [Jonathan Ragan-Kelley](/people/j/jonathan-ragan-kelley/)
              [Eric P. Xing](/people/e/eric-xing/)
              [Yoon Kim](/people/y/yoon-kim/)
              [Findings of the Association for Computational Linguistics: EMNLP 2024](/volumes/2024.findings-emnlp/)
            The deployment of large language models (LLMs) is often constrained by memory bandwidth, where the primary bottleneck is the cost of transferring model parameters from the GPUâ€™s global memory to its registers. When coupled with custom kernels that fuse the dequantization and matmul operations, weight-only quantization can thus enable faster inference by reducing the amount of memory movement. However, developing high-performance kernels for weight-quantized LLMs presents substantial challenges, especially when the weights are compressed to non-evenly-divisible bit widths (e.g., 3 bits) with non-uniform, lookup table (LUT) quantization. This paper describes FLUTE, a flexible lookup table engine for LUT-quantized LLMs, which uses offline restructuring of the quantized weight matrix to minimize bit manipulations associated with unpacking, and vectorization and duplication of the lookup table to mitigate shared memory bandwidth constraints. At batch sizes < 32 and quantization group size of 128 (typical in LLM inference), the FLUTE kernel can be 2-4x faster than existing GEMM kernels. As an application of FLUTE, we explore a simple extension to lookup table-based NormalFloat quantization and apply it to quantize LLaMA3 to various configurations, obtaining competitive quantization performance against strong baselines while obtaining an end-to-end throughput increase of 1.5 to 2 times.
          2022
                [Efficient Task-Oriented Dialogue Systems with Response Selection as an Auxiliary Task](/2022.icnlsp-1.2/)
              [Todor Kolev](/people/t/todor-kolev/)
              [Proceedings of the 5th International Conference on Natural Language and Speech Processing (ICNLSP 2022)](/volumes/2022.icnlsp-1/)
            Search
                Co-authors
                    1
                  show all...
                      [Eric Xing](/people/e/eric-xing/)
                Venues
                    [findings](/venues/findings/)
                    [icnlsp](/venues/icnlsp/)
            Fix data
        [Creative Commons Attribution-NonCommercial-ShareAlike 3.0 International License](https://creativecommons.org/licenses/by-nc-sa/3.0/)
        . Permission is granted to make copies for the purposes of teaching and research. Materials published in or after 2016 are licensed on a
        [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/)
        .
      The ACL Anthology is managed and built by the
        [ACL Anthology team](/info/credits/)
        of volunteers.
        Site last built on 22 February 2025 at 20:15 UTC with
          [commit 68f8aca](https://github.com/acl-org/acl-anthology/tree/68f8aca8a464a354e8f09be090d67a8f482566f2)