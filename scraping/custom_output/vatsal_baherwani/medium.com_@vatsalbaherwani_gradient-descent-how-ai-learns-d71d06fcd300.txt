            [Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2Fd71d06fcd300&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---top_nav_layout_nav-----------------------------------------)
                  Sign up
                    [Sign in](/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40vatsalbaherwani%2Fgradient-descent-how-ai-learns-d71d06fcd300&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)
                      Write
                              Gradient Descent: How AI “learns”
                                                          [Vatsal Baherwani](/@vatsalbaherwani?source=post_page---byline--d71d06fcd300---------------------------------------)
                                                    ·
                                                      [Follow](/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F7f56add12bef&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40vatsalbaherwani%2Fgradient-descent-how-ai-learns-d71d06fcd300&user=Vatsal+Baherwani&userId=7f56add12bef&source=post_page-7f56add12bef--byline--d71d06fcd300---------------------post_header------------------)
                                                  7 min read
                                                  Nov 20, 2024
                                                --
                                                            Listen
                                                  Share
                              This is my attempt at explaining a not yet fully understood concept to a general audience. I will probably oversimplify or misrepresent some details.
                            How do humans learn?
                            How would you teach a computer to speak English? Perhaps a good starting point for this problem is to look at how humans learn to speak. For us, this process starts during infancy when we observe the people closest to us (e.g. parents) speaking. We learn to mimic their words at first before we actually understand what they mean. Then we begin to form our own sentences and express our own thoughts. Generally, the more exposure we get to a certain language, the better we can comprehend and articulate conversations in that language.
                            There’s a few fundamental caveats in translating this process into something a computer can perform. The most obvious one is translating human language into computer language. Computers only process numbers. Everything that’s not a number is secretly abstracted by numbers under the hood. For example, when I upload this article, the raw uploaded data will be literal numbers corresponding to the letters in this text (like 0 for ‘a’, 1 for ‘b’, and so on). But for the sake of simplicity, I won’t go into how language is converted into a format that an AI model can learn from. Let’s just assume we have a means of turning real human text into a numerical representation, and vice versa.
                            How could an AI learn?
                            So how do we get the AI to learn to speak like a human? It sounds like a good idea to get an AI model that can mimic existing human language, similar to how infants start. Assume we have access to all the publicly available text on Internet — modern AI models are in fact trained over this entire set. And let’s also assume that someone has magically cleaned all the junk out of this data: no URLs, no gibberish, no spam, just plain old English (in reality, this is a painstakingly tedious process). We want the AI model to spit out text that mimics this human text from the Internet. How do we explicitly define this goal?
                            Let’s break this down into parts: the model should spit out text, the text should be in English, and the English should resemble the way humans speak. The first step is trivial; we can just define our model to output one of many possible letters, numbers, punctuation etc. But a dumb model that just makes random decisions will give you a garbled pile of text that either doesn’t qualify as English at all, or doesn’t sound like a human. We want to avoid this. We want the AI model’s text to look as much like English as possible, and sound as convincingly human as possible. But as previously mentioned, computers really only understand numbers. So whatever method we employ has to be strictly numerical. We can do this by thinking about English as a probability distribution.
                              Human language is a constantly evolving distribution. For example, before the 1900s the word “virus” was only used in the context of a pathogen. The adoption of computers led the word to also take on the definition of malware. Now, you are several times more likely to see this word in a book (and to use it in conversation). Source: Google Books Ngram Viewer
                            If it’s about to rain, what are you more likely to tell a friend? “It feels like it’s about to rain”, or “I smell petrichor”? One of these responses is much more common than the other. And there are some other responses that would just not fit the context at all, like “I sure do love the taste of turmeric”. We want to instill this concept into an AI model as well. Talking like a human means using language that is common given the context. For an AI model that thinks numerically, let’s assume our real distribution of human conversations has something like a 90% chance of responding with “It feels like it’s about to rain”, a 10% chance of saying “I smell petrichor”, and a 0% chance of saying “I sure do love the taste of turmeric”. We want the AI model to match this as closely as possible. In numerical terms, this means
                              minimizing the difference between the distribution of human language and the distribution of the AI model’s responses.
                            Learning the Objective with Gradient Descent
                            Now that we have an objective, we need to get our AI to learn from it. Assume our AI model takes in some text as numbers, does a bunch of math, and responds with its own text. How can we minimize the difference between its distribution of possible responses and the distribution of human responses? Let’s start with an analogous example.
                            Imagine you’re blindfolded at the summit of a mountain, and your goal is to get back to the bottom. In other words, you are at some really high elevation, and you want to lower it as much as possible. How would you do this? For starters, you could at least get off the peak by taking a step in any direction. Now, you’re still blindfolded, but your feet are slanted since you’re on the slope of the mountain. How do you know where to step next?
                            Notice how when your feet are slanted on this mountain, you have a sense of which direction moves you downwards. When you’re walking forwards down a hill, the front of your feet step lower than your heels. If you instead shuffle rightwards down this hill, the right side of each foot will strike the ground lower than the left. When blindfolded, you can apply this knowledge to conclude the direction that will bring you downward. If the right side of your foot feels like it’s slanting lower than the left side, you know that the direction to your right is the way down. If every step is bringing you slightly downward each time, you can be sure that eventually you will reach the lowest point on the mountain.
                            So, what’s with the blindfolding? The motivation behind this example is to avoid using our high-level human senses and break down the problem of minimizing a value into the concept of slopes. We wanted to minimize our elevation on this mountain, so we used the slopes of our feet to guide this descent. Training an AI model is (at a very high level) a similar concept.
                            AI models process information with
                              parameters
                              , which is just a fancy word for numbers that define the model. A simple example: if you want to make an AI model that multiplies every number by 2, it will just have one parameter — the number 2. When you give the model a number, it will multiply this parameter with that number to give you the result. Language is a much more difficult concept. Let’s say you give the model some text, in a numerical format. Current AI language models have on the order of 100 billion parameters. So this model will take the text, and use each of its parameters to do some math with it: addition, multiplication, exponentiation, etc.
                            Let’s say we asked our model about the weather, and it came up with this distribution: 27% chance of “It feels like it’s about to rain”, 16% chance of “I smell petrichor”, and a 57% chance of “I sure do love the taste of turmeric”. This is way different from the actual human distribution, and we can calculate a single solitary number that describes this difference (see
                              [cross entropy](https://en.wikipedia.org/wiki/Cross-entropy)
                              for more details). We want to minimize this difference. Since all we did was math, we can look back on the AI model’s parameters and say, for example, that adjusting some parameters would decrease the chance of the third response and bring us closer to the human distribution. This is similar to being on the mountain, where we could look at the slope of our feet and move in the according direction to take us downward. When going down the mountain blindfolded, we really only had to think about two dimensions: forward/backward, and left/right. Now, we can increase or decrease each of these parameters, which gives us 100 billion possible dimensions to consider. The word “slope” isn’t really applicable here, so mathematicians refer to this concept as
                              gradients
                              .
                            Let’s summarize how we’re getting this AI model to learn: we’re looking at its current output and seeing how far we are from the distribution of human English language. Then, we’re computing the
                              gradient
                              of this distance, with respect to the model parameters. Technically, the gradient gives us the direction of upward ascent — moving forward in the direction of the gradient will increase our distance from the human distribution. We want to minimize the distance, so we do the opposite i.e. move in the opposite direction of the gradient. Computing the gradient + descending in the opposite direction =
                              gradient descent
                            Real Applications
                            Creating an AI model with a couple hundred billion parameters and minimizing this objective of mimicking human English as closely as possible is how models like ChatGPT are trained. This is a huge oversimplification, but the process involves the same high-level concept of defining a goal, computing the proximity of the current AI model to that goal, and applying gradient descent to get closer to the goal. There’s some extra bells and whistles for language models that do not involve gradient descent, like ensuring the model is not toxic or rude (concepts that are difficult to define numerically). Moreover, image and video generation models follow the similar pattern of looking to minimize the chance that a generated output looks as realistic and human as possible.
                            But this method is not foolproof. Stepping around a 100 billion dimensional mountain leads to all sorts of convoluted problems like falling off a cliff, running into a wall, or simply just getting stuck. AI practitioners deal with this by very carefully adjusting the model to take miniscule steps to maintain a stable trajectory. But researchers still don’t fully understand the details of how gradient descent works in these high dimensions or why it’s so complicated. So far, however, it’s the most viable method for training large AI models. And it’s gotten us pretty far.
                    AI
                    Language
                          Written by
                            [2 Followers](/@vatsalbaherwani/followers?source=post_page---post_author_info--d71d06fcd300---------------------------------------)
                          [1 Following](/@vatsalbaherwani/following?source=post_page---post_author_info--d71d06fcd300---------------------------------------)
                        My personal page:
                          [https://vatsal0.github.io/](https://vatsal0.github.io/)
                  No responses yet
                        Help
                        Status
                        About
                        Careers
                        Press
                        Blog
                        Privacy
                        Terms
                        Text to speech
                        Teams