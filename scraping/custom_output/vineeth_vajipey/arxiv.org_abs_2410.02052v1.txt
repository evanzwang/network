      start desktop header
          We gratefully acknowledge support from the Simons Foundation,
            [member institutions](https://info.arxiv.org/about/ourmembers.html)
            , and all contributors.
          [Donate](https://info.arxiv.org/about/donate.html)
          >
          [cs](/list/cs/recent)
          arXiv:2410.02052v1
                  [Help](https://info.arxiv.org/help)
                  |
                  [Advanced Search](https://arxiv.org/search/advanced)
                    All fields
                    Title
                    Author
                    Abstract
                    Comments
                    Journal reference
                    ACM classification
                    MSC classification
                    Report number
                    arXiv identifier
                    DOI
                    ORCID
                    arXiv author ID
                    Help pages
                    Full text
              Search
      /end desktop header
                open search
                  GO
                open navigation menu
                quick links
                    [Login](https://arxiv.org/login)
                    [Help Pages](https://info.arxiv.org/help)
                    [About](https://info.arxiv.org/about)
      /end mobile-header
        rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2410.02052"
        dc:identifier="/abs/2410.02052"
        dc:title="Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning"
        trackback:ping="/trackback/2410.02052" />
    </rdf:RDF>
              Computer Science > Computation and Language
              (cs)
                [Submitted on 2 Oct 2024 (this version),
                  latest version 13 Jan 2025
                  (
                  [v4](https://arxiv.org/abs/2410.02052v4)
                  )]
                  Title:
                  Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning
                  Authors:
                  [Xiao Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+X)
                  ,
                  [Baolin Peng](https://arxiv.org/search/cs?searchtype=author&query=Peng,+B)
                  [Vineeth Vajipey](https://arxiv.org/search/cs?searchtype=author&query=Vajipey,+V)
                  [Hao Cheng](https://arxiv.org/search/cs?searchtype=author&query=Cheng,+H)
                  [Michel Galley](https://arxiv.org/search/cs?searchtype=author&query=Galley,+M)
                  [Jianfeng Gao](https://arxiv.org/search/cs?searchtype=author&query=Gao,+J)
                  [Zhou Yu](https://arxiv.org/search/cs?searchtype=author&query=Yu,+Z)
                View a PDF of the paper titled Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning, by Xiao Yu and 5 other authors
                [View PDF](/pdf/2410.02052v1)
                [HTML (experimental)](https://arxiv.org/html/2410.02052v1)
                  Abstract:
                  Autonomous agents have demonstrated significant potential in automating complex multistep decision-making tasks. However, even state-of-the-art vision-language models (VLMs), such as GPT-4o, still fall short of human-level performance, particularly in intricate web environments and long-horizon planning tasks. To address these limitations, we introduce Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to enhance the ability of AI agents, e.g., powered by GPT-4o, to explore decision space on the fly. R-MCTS extends traditional MCTS by 1) incorporating contrastive reflection, allowing agents to learn from past interactions and dynamically improve their search efficiency; and 2) using multi-agent debate to provide reliable state evaluation. Moreover, we improve the agent's performance by fine-tuning GPT-4o through self-learning, using R-MCTS generated tree traversals without any human-provided labels. On the challenging VisualWebArena benchmark, our GPT-4o-based R-MCTS agent achieves a 6% to 30% relative improvement across various tasks compared to the previous state-of-the-art. Additionally, we show that the knowledge gained from test-time search can be effectively transferred back to GPT-4o via fine-tuning. The fine-tuned GPT-4o matches 97% of R-MCTS's performance while reducing compute usage by a factor of four at test time. Furthermore, qualitative results reveal that the fine-tuned GPT-4o model demonstrates the ability to explore the environment, evaluate a state, and backtrack to viable ones when it detects that the current state cannot lead to success. Moreover, our work demonstrates the compute scaling properties in both training - data collection with R-MCTS - and testing time. These results suggest a promising research direction to enhance VLMs' reasoning and planning capabilities for agentic applications via test-time search and self-learning.
                CONTEXT
                        Subjects:
                          Computation and Language (cs.CL)
                          ; Computer Vision and Pattern Recognition (cs.CV)
                        Cite as:
                            [arXiv:2410.02052](https://arxiv.org/abs/2410.02052)
                            [cs.CL]
                        (or
                          for this version)
                          [https://doi.org/10.48550/arXiv.2410.02052](https://doi.org/10.48550/arXiv.2410.02052)
                              Focus to learn more
                            tooltip description
              Submission history
              From: Xiao Yu [
              [view email](/show-email/0f34d21f/2410.02052)
              ]
              [v1]
              Wed, 2 Oct 2024 21:42:35 UTC (6,770 KB)
                [[v2]](/abs/2410.02052v2)
              Tue, 15 Oct 2024 14:59:46 UTC (8,143 KB)
                [[v3]](/abs/2410.02052v3)
              Fri, 18 Oct 2024 03:27:37 UTC (8,144 KB)
                [[v4]](/abs/2410.02052v4)
              Mon, 13 Jan 2025 19:51:53 UTC (8,160 KB)
          end leftcolumn
              Full-text links:
              Access Paper:
                  [Other Formats](/format/2410.02052v1)
                [view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/)
            end full-text
            Current browse context:
              cs.CL
                  [< prev](/prevnext?id=2410.02052&function=prev&context=cs.CL)
                  [next >](/prevnext?id=2410.02052&function=next&context=cs.CL)
                [new](/list/cs.CL/new)
                [recent](/list/cs.CL/recent)
                [2024-10](/list/cs.CL/2024-10)
              Change to browse by:
                  [cs.CV](/abs/2410.02052?context=cs.CV)
              References & Citations
                  [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2410.02052)
                  [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2410.02052)
                  [Semantic Scholar](https://api.semanticscholar.org/arXiv:2410.02052)
              export BibTeX citation
              Loading...
                  BibTeX formatted citation
                  ×
                  loading...
                  Data provided by:
                Bookmark
          end extra-services
          LABS AREA
              Bibliographic Tools
                Bibliographic and Citation Tools
                        Bibliographic Explorer Toggle
                      Bibliographic Explorer
                        [What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer)
                        )
                        Connected Papers Toggle
                      Connected Papers
                        [What is Connected Papers?](https://www.connectedpapers.com/about)
                        Litmaps Toggle
                      Litmaps
                        [What is Litmaps?](https://www.litmaps.co/)
                        scite.ai Toggle
                      scite Smart Citations
                        [What are Smart Citations?](https://www.scite.ai/)
              Code, Data, Media
                Code, Data and Media Associated with this Article
                        alphaXiv Toggle
                      alphaXiv
                        [What is alphaXiv?](https://alphaxiv.org/)
                        Links to Code Toggle
                      CatalyzeX Code Finder for Papers
                        [What is CatalyzeX?](https://www.catalyzex.com)
                        DagsHub Toggle
                      DagsHub
                        [What is DagsHub?](https://dagshub.com/)
                        GotitPub Toggle
                      Gotit.pub
                        [What is GotitPub?](http://gotit.pub/faq)
                        Huggingface Toggle
                      Hugging Face
                        [What is Huggingface?](https://huggingface.co/huggingface)
                      Papers with Code
                        [What is Papers with Code?](https://paperswithcode.com/)
                        ScienceCast Toggle
                      ScienceCast
                        [What is ScienceCast?](https://sciencecast.org/welcome)
              Demos
                        Replicate Toggle
                      Replicate
                        [What is Replicate?](https://replicate.com/docs/arxiv/about)
                        Spaces Toggle
                      Hugging Face Spaces
                        [What is Spaces?](https://huggingface.co/docs/hub/spaces)
                      TXYZ.AI
                        [What is TXYZ.AI?](https://txyz.ai)
              Related Papers
                Recommenders and Search Tools
                        Link to Influence Flower
                      Influence Flower
                        [What are Influence Flowers?](https://influencemap.cmlab.dev/)
                        Core recommender toggle
                      CORE Recommender
                        [What is CORE?](https://core.ac.uk/services/recommender)
              About arXivLabs
                    arXivLabs: experimental projects with community collaborators
                    arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.
                    Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.
                    Have an idea for a project that will add value for arXiv's community?
                        Learn more about arXivLabs
                      .
          END LABS AREA
            [Which authors of this paper are endorsers?](/auth/show-endorsers/2410.02052)
            [What is MathJax?](https://info.arxiv.org/help/mathjax.html)
        Macro-Column 1
                    contact arXiv
                    Click here to contact arXiv
                  [Contact](https://info.arxiv.org/help/contact.html)
                    subscribe to arXiv mailings
                    Click here to subscribe
                  [Subscribe](https://info.arxiv.org/help/subscribe)
        End Macro-Column 1
        Macro-Column 2
                  [Copyright](https://info.arxiv.org/help/license/index.html)
                  [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)
                  [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
                    [arXiv Operational Status](https://status.arxiv.org)
                    or
        end MetaColumn 2
        End Macro-Column 2