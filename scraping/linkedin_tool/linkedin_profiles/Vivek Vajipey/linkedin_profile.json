[
  {
    "fullName": "Vivek Vajipey",
    "linkedin_internal_id": "893601052",
    "first_name": "Vivek",
    "last_name": "Vajipey",
    "public_identifier": "vivek-vajipey",
    "background_cover_image_url": "https://static.licdn.com/aero-v1/sc/h/5q92mjc5c51bjlwaj3rs9aa82",
    "profile_photo": "https://media.licdn.com/dms/image/v2/D5603AQET6mgXE42DNw/profile-displayphoto-shrink_200_200/profile-displayphoto-shrink_200_200/0/1719107890138?e=2147483647&v=beta&t=oycbSSqTgvtWznKQ1MSh1Ut2y48zsfE21OMiOOk7uS4",
    "headline": "Student at Stanford University",
    "location": "482 followers\n          \n          \n              479 connections",
    "about": "",
    "experience": [
      {
        "position": "Researcher",
        "company_url": "https://www.linkedin.com/company/stanford-ai-lab?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/C560BAQHIr1u04mwtMA/company-logo_100_100/company-logo_100_100/0/1631434386745/stanford_ai_lab_logo?e=2147483647&v=beta&t=gsTlxGroppn7t9FHXDLz-v5YIFFSxzyhZbN-8iNrASs",
        "company_name": "Stanford Artificial Intelligence Laboratory (SAIL)",
        "location": null,
        "summary": "Researching reasoning in large language models. (Computation & Cognition Lab)",
        "starts_at": "2023",
        "ends_at": "Present",
        "duration": "2 years"
      },
      {
        "position": "Fellow",
        "company_url": "https://www.linkedin.com/company/true-ventures?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/D4E0BAQH3s3VCt4nA0Q/company-logo_100_100/company-logo_100_100/0/1719255530411/true_ventures_logo?e=2147483647&v=beta&t=UM_MP3pIA540-7WXKJDwZQ8rS0b1PSatv5X3GmYW_pE",
        "company_name": "True Ventures",
        "location": null,
        "summary": "",
        "starts_at": "Apr 2024",
        "ends_at": "Aug 2024",
        "duration": "5 months"
      },
      {
        "position": "VP of Events",
        "company_url": "https://www.linkedin.com/company/stanford-energy-club?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/C4E0BAQHdUFqtoYnxjA/company-logo_100_100/company-logo_100_100/0/1631326488419?e=2147483647&v=beta&t=La5pmQ6Foz4MQo8ZHzSuNwB__Pa-D9m1AiDfEfbem9w",
        "company_name": "Stanford Energy Club",
        "location": null,
        "summary": "",
        "starts_at": "Nov 2022",
        "ends_at": "Mar 2024",
        "duration": "1 year 5 months"
      },
      {
        "position": "Researcher",
        "company_url": "https://www.linkedin.com/school/stanford-university/?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/C560BAQHr9suxyJBXMw/company-logo_100_100/company-logo_100_100/0/1635534378870/stanford_university_logo?e=2147483647&v=beta&t=ZvB25L95o9w4q9drvsWxGcM49tX66Cf5LsLAxYp8rSs",
        "company_name": "Stanford University",
        "location": null,
        "summary": "Machine learning with remote sensing data for agriculture applications. (Lobell Lab)",
        "starts_at": "May 2022",
        "ends_at": "Sep 2022",
        "duration": "5 months"
      },
      {
        "position": "Software Engineer",
        "company_url": "https://www.linkedin.com/company/swingtechconsulting?trk=public_profile_experience-item_profile-section-card_subtitle-click",
        "company_image": "https://media.licdn.com/dms/image/v2/C560BAQFDCEpjg7XVoQ/company-logo_100_100/company-logo_100_100/0/1630670164797/swingtechconsulting_logo?e=2147483647&v=beta&t=TxjmmRYX68jPmdRpFIwMQhzLMXzQhkZ2lVL__YL4PzU",
        "company_name": "Swingtech",
        "location": null,
        "summary": "",
        "starts_at": "May 2021",
        "ends_at": "Aug 2021",
        "duration": "4 months"
      }
    ],
    "education": [
      {
        "college_url": "https://www.linkedin.com/school/stanford-university/?trk=public_profile_school_profile-section-card_image-click",
        "college_name": "Stanford University",
        "college_image": "https://media.licdn.com/dms/image/v2/C560BAQHr9suxyJBXMw/company-logo_100_100/company-logo_100_100/0/1635534378870/stanford_university_logo?e=2147483647&v=beta&t=ZvB25L95o9w4q9drvsWxGcM49tX66Cf5LsLAxYp8rSs",
        "college_degree": "Master of Science - MS",
        "college_degree_field": "Computer Science",
        "college_duration": "2024 - 2025",
        "college_activity": ""
      },
      {
        "college_url": "https://www.linkedin.com/school/stanford-university/?trk=public_profile_school_profile-section-card_image-click",
        "college_name": "Stanford University",
        "college_image": "https://media.licdn.com/dms/image/v2/C560BAQHr9suxyJBXMw/company-logo_100_100/company-logo_100_100/0/1635534378870/stanford_university_logo?e=2147483647&v=beta&t=ZvB25L95o9w4q9drvsWxGcM49tX66Cf5LsLAxYp8rSs",
        "college_degree": "Bachelor of Science - BS",
        "college_degree_field": "Computer Science",
        "college_duration": "2021 - 2025",
        "college_activity": "Geophysics Minor"
      },
      {
        "college_url": "https://www.linkedin.com/school/west-windsor-plainsboro-high-school-north/?trk=public_profile_school_profile-section-card_image-click",
        "college_name": "West Windsor-Plainsboro High School North",
        "college_image": "https://media.licdn.com/dms/image/v2/C4E0BAQFpBcKS1KG64g/company-logo_100_100/company-logo_100_100/0/1630583432654?e=2147483647&v=beta&t=DhGQe40K0vXbjaVRkY0O1tb1OlQZy3aNEP-2PgyWeAU",
        "college_degree": "",
        "college_degree_field": null,
        "college_duration": "2017 - 2021",
        "college_activity": ""
      }
    ],
    "articles": [],
    "description": {
      "description1": "Stanford Artificial Intelligence Laboratory (SAIL)",
      "description1_link": "https://www.linkedin.com/company/stanford-ai-lab?trk=public_profile_topcard-current-company",
      "description2": "Stanford University",
      "description2_link": "https://www.linkedin.com/school/stanford-university/?trk=public_profile_topcard-school",
      "description3": ""
    },
    "activities": [],
    "volunteering": [],
    "certification": [],
    "people_also_viewed": [
      {
        "link": "https://www.linkedin.com/in/sam-farid-0b87219a?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Sam Farid\n        \n              \n          This is a fascinating paper evaluating DeepMind\u2019s AlphaFold 3 and its approach to predicting ligand docking: https://lnkd.in/gieM2nW5\n\nThe authors used a clever method to evaluate AlphaFold, finding that it can make glaring mistakes when placing molecules in binding sites because its model is likely only fitted to the protein\u2019s backbone geometry instead of actually learning the underlying physical interactions.\n\nNot that AlphaFold isn\u2019t a breakthrough technology for drug discovery, but this paper highlights that even the most cutting-edge applications of AI are perhaps still best used to generate candidates, but humans still need to perform robust tests to validate them.\n\nWe\u2019ve been hearing a similar sentiment from nearly all of our users when bringing AI into cloud infrastructure management. Flightcrew has been read-only from the start, using AI to generate insights but requiring a human to sign off before any changes are applied to their infrastructure. The goal is to speed up existing workflows, not create a new point of failure.\n\nDrug discovery and managing Kubernetes are completely different applications but we\u2019re all similarly wary of trusting full AI automation and at this rate, likely will be for some time.",
        "location": "19"
      },
      {
        "link": "https://in.linkedin.com/in/sujaysriv?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Sujay Srivastava\n        \n              \n          Ilya Sutskever confirmed at NeurIPS '24 pre-training scaling law will not be the same. Internet data, the AI fossil fuel, is running out.\nIt is not the end of the AI hype, the rapid rise of foundational AI has outpaced our ability to fully grasp their long-term societal impact. AI agents and applications are poised to transform every facet of our lives, and we will see it happening in coming few years.\nThere will still be improvements in model architecture, and synthetic data creation, so models will improve, just not at the lightning speed they did till now. \nLooking forward, the single most important milestone in AI is creating systems capable of advancing themselves\u2014AI researchers that can innovate and improve foundational models autonomously. We need to assess how far are we from this stage.",
        "location": "9"
      },
      {
        "link": "https://www.linkedin.com/in/ajayso?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Ajay S.\n        \n              \n          Implementing Reinforcement Learning (RL) in the context of Large Language Models (LLMs) posed challenges, particularly in designing reward engineering and penalization components. In traditional RL, reward engineering is straightforward, often defined through a scalar reward function. However, implementing the same concept in RL with LLMs required a critic model that evaluates the questions and answers, critiquing them in a manner that simulates the reward function.\n\nA recent research paper on Natural Language Reinforcement Learning (NLRL) provides an intriguing perspective on the reward problem in RL by translating traditional RL concepts into natural language constructs. Here's a summary of how it addresses the reward problem:\nLanguage-Based Value Functions:\nUnlike traditional RL, which uses scalar rewards, NLRL employs language-based value functions. These functions articulate the effectiveness of policies and actions in a more interpretable, natural language format. This approach addresses the need for richer, more informative feedback than simple numerical scores.\n\nIterative Refinement of Language Policies and Critics:\nNLRL incorporates a structured training pipeline where language-based policies and critics are iteratively refined using textual feedback. This method allows the system to handle the complexities of defining rewards in natural language, improving its understanding and evaluation of actions over time through textual feedback loops.\n\nUse of Large Language Models (LLMs):\nThe framework leverages the advanced capabilities of LLMs to interpret, generate, and evaluate complex language-based information. This is essential for implementing language-based value functions and policies, enabling the system to generate and refine responses that align closely with desired outcomes.\n\nDistilling Enhanced Actions and Evaluations:\nNLRL employs a process where enhanced evaluations from the language-based value function approximator and improved actions from the language policy improvement operator are distilled. This allows for continuous refinement and learning.\n\nCompatibility with Existing LLMs:\nThe framework is designed to be compatible with current LLMs, enabling it to leverage pre-existing language models to process and generate linguistic data effectively. This compatibility is crucial for translating the traditional reward engineering process into a language-based framework.\n\nThe NLRL framework's introduction of natural language as a medium for representing and evaluating policies and rewards offers a novel solution to the reward problem in RL. By shifting from numerical reward systems to a more flexible, expressive, and intuitive system based on language, NLRL enhances interpretability and adaptability, paving the way for more diverse RL applications.\nInnovation Hacks AI Inc.\n#llm #refincorcementlearning #ai",
        "location": "4"
      },
      {
        "link": "https://www.linkedin.com/in/blabs?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Brayden Levangie\n        \n              \n          AI is only going to get cheaper and better",
        "location": "1"
      },
      {
        "link": "https://nl.linkedin.com/in/vyacheslavtikhonov?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Vyacheslav Tykhonov\n        \n              \n          Yes, the future is AI Agents navigating the Web in the decentralised manner, using Croissant for Machine Learning to check content ownership and take licenses into account. It's a kind of \"TomTom\" for the future Web, represented as moving knowledge graph.\n\nWe knew all of that for years and it's coming.",
        "location": "2\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                1 Comment"
      },
      {
        "link": "https://bz.linkedin.com/in/omarsar?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Elvis S.\n        \n              \n          Not sure why LLM compression is not discussed more often. Leveraging very large language models to build capable smaller LMs will be a huge win for everyone. \n\nCompanies like NVIDIA and Meta seem to be pushing hard on pruning and distillation for good reason. The compressed Llama 3.1 and NeMo models mentioned in this paper look very competitive.",
        "location": "472\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                7 Comments"
      },
      {
        "link": "https://nl.linkedin.com/in/sunxcint?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Tao Sun\n        \n              \n          We are thrilled to announce our latest paper, now available on [arXiv](https://lnkd.in/grgvEftD)! \ud83d\udcda\n\n\ud83d\udd2c Title: DPSNN: Spiking Neural Network for Low-Latency Streaming Speech Enhancement\n\nIn our recent work, we tackle a crucial issue in speech enhancement SNN solutions: latency. While neuromorphic hardware is designed for low latency, some algorithms and implementations can still introduce significant delays. In speech enhancement, the Short-Time Fourier Transform (STFT)\u2014a common preprocessing step in frequency-domain approaches\u2014can be a significant source of latency. Inspired by the success of high-performance, low-latency deep learning models, we have developed a novel time-domain SNN framework that achieves the very low latency required for applications like hearing aids.\n\nKey Contributions of Our Paper:\n1. Innovative Solution: We introduce a novel two-phase time-domain streaming SNN framework that effectively addresses latency while ensuring high accuracy and power efficiency.\n2. Latency Optimization: Traditional methods often suffer from latency due to long sampling windows, such as 32ms. Our time-domain approach significantly reduces this latency, meeting the stringent requirements of real-time applications like hearing aids, which demand latencies under 5ms.\n3. Competitive Performance: Our framework not only reduces latency but also achieves competitive performance compared to current SNN models, pushing the boundaries of what\u2019s possible in speech enhancement.\n\nExplore the full details of our work on \n[arXiv](https://lnkd.in/grgvEftD) and discover how our innovations are advancing the practical applications of neuromorphic computing in this vital field.\n\nWe look forward to your feedback and discussions!\n\n#SpeechEnhancement #SNN #LatencyReduction #DeepLearning #NeuralNetworks #RealTimeProcessing #AI #TechInnovation #arXiv #HearingAids",
        "location": "25\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                2 Comments"
      },
      {
        "link": "https://il.linkedin.com/in/roy-nissim?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Roy Nissim\n        \n              \n          Hugging Face demonstrated test-time scaling with Llama 1B, turning it into a \"reasoner\" that outperformed Llama 8B (8x larger!) in math.\n\nTest-time scaling has opened up a whole new dimension in model performance and AI research.\u00a0I wrote about this when OpenAI released its o-1 model (the first 'reasoner' model). You can read it here: https://lnkd.in/gi737nsA\n\nAt its core, a reasoner model uses Chain of Thought (CoT) reasoning\u2014breaking problems into logical steps, solving them sequentially, and backtracking when needed. Scaling test-time compute enables this reasoning through two techniques:\n\n1/ Self-Refinement: Models iteratively improve outputs, correcting errors over multiple passes.\n\n2/ Search & Verifiers: Generate multiple outputs and evaluate them with verifier models (e.g., Process Reward Models for steps, Outcome Reward Models for final answers).\n\nHugging Face\u2019s contribution stands out because they\u2019ve shown test-time scaling can be applied to any model, even smaller, open-source ones. They\u2019ve also made it easier for others\u00a0to replicate and advance the technique\u2014helping the OS community catch up.\n\nThat said, inference scaling is still far from efficient. Llama 1B required ~20 reasoning steps to outperform Llama 8B, a model only 8x larger. Nevertheless, inference scaling already achieves two critical things:\n\n\u2013 Pushing the frontier: Scaling inference pushes the boundaries of accuracy and delivers better results.\n\u2013 Smaller hardware requirement: Smaller models can match the performance of larger ones, reducing reliance on expensive infrastructure.\n\nThe next frontier isn\u2019t just about bigger models\u2014it's about smarter inference. Excited to see where this leads!",
        "location": "62\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                4 Comments"
      },
      {
        "link": "https://in.linkedin.com/in/tanmaydhote?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Tanmay Dhote\n        \n              \n          Yann LeCun recently shared an insightful piece of advice for students aspiring to shape the future of AI:\n\n\"Don't work on LLMs. This domain is dominated by large companies with massive capital and data resources. Instead, focus on developing next-gen AI systems that overcome the limitations of current LLMs.\"\n\nAs we acknowledge the vast resources required for LLM development, it's crucial to explore alternative methods. \n\nWhat could these next-gen AI systems look like?",
        "location": "17\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                1 Comment"
      },
      {
        "link": "https://www.linkedin.com/in/vkleban?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Vitaly Kleban\n        \n              \n          Cost-aware inference is important, but currently LLM prompts are not interoperable. Prompts that work good on one LLM may show worse performance on the other, even more advanced LLM or even on the different versions of the same LLM.\n\nOnce interoperability problem is solved it will also be possible to implement cost-aware inference that strategically manages computational resources for prompt processing, ensuring sustainable costs. \n\nThis approach includes model routing https://www.notdiamond.ai/ - automatically selecting the most appropriate model for each task. Startups in this area are actively supported by the industry veterans.",
        "location": "4\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                2 Comments"
      },
      {
        "link": "https://sg.linkedin.com/in/jeffreypaine?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Jeffrey Paine\n        \n              \n          Here are the top 3 most promising startup ideas I identified from the interview with John Schulman (OpenAI) by Dwarkesh Patel.\n\n1.  AI-powered coding assistant that can understand high-level instructions, write code, test it, and iterate to complete entire coding projects with minimal human involvement. e.g. Devin, Github Co-pilot\n\n2. AI system that can read and understand all the scientific literature in a field, sift through data, and help accelerate research by assisting human scientists and making new discoveries. e.g. Iris.ai\n\n3. AI assistant that works alongside a human on their computer, understands the full context of the project they are working on, proactively makes suggestions, and performs work in the background. e.g. Adept AI\n\nhttps://lnkd.in/d5umCuhd",
        "location": "41\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                6 Comments"
      },
      {
        "link": "https://www.linkedin.com/in/srinipagidyala?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Srini Pagidyala\n        \n              \n          Ilya like all other LLM vendors is now actively trying to cover up the 10s of billion$ in LLM tech debt by claiming \u201cAgentic AI\u201d as the new panacea to solve intelligence - even lowering the bar for AGI. \n\nYou can't brute force your way to solve intelligence.\n\nThere is no \"i\" in AI without \"Cognition\".\n\nSelf-learning and adapting in real-time isn't going to happen with LLMs or the wrapper \"Agents\" sitting on top of these LLMs. \n\nHopefully they are not going to sit on \"Agentic AI\" for another few years to arrive at this conclusion as they did with the scaling laws.\n\nWe need a Neuro-Symbolic approach centered around human-like Cognition to solve intelligence that doesn\u2019t require trillions of data points, billion$ in computational resources and even more $$ in energy requirements while it cannot learn & adapt continuously yet after all this it hallucinates. INSA is the exact opposite of this. \n\nINSA: Integrated Neuro-Symbolic Architecture\nhttps://lnkd.in/ggFTxuUz\n\nThe insanity of huge language models: \nhttps://lnkd.in/d5pcyf6R",
        "location": "32\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                11 Comments"
      },
      {
        "link": "https://uk.linkedin.com/in/harrystebbings?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Harry Stebbings\n        \n              \n          I\u2019ve been fortunate enough to do AI shows with Sam Altman, Yann LeCun & Arthur Mensch.\n\nBut this show today with David Cahn at Sequoia Capital is the best of all of them:\n\n\ud83e\udde0 AI\u2019s $600BN question\n\n\ud83c\udfc3 Why no one will never train a frontier model on the same data center twice\n\n\ud83c\udfd7\ufe0f Servers, steel & power: The 3 pillars of the future of AI\n\nNot one to be missed, my 8 key takeaways \ud83d\udc47\n\n1. The Oligopoly That is Blowing Billions on AI \n\n- Microsoft, Azure, & Google now represent $7T market cap.\n- Of course they will spend billions to protect their oligopoly\n- What is most interesting is Meta is the only incumbent that is financing this not with cloud being their cash cow.\n\n2. How Startups Could Win with the Cost of Compute Coming Down\n\n- Big tech companies are producers of compute & startups are consumers of compute.\n- Overproduction of compute = lower prices.\u00a0\n- This gives startups higher margins & makes them more valuable. \n\n3. Is Compute the Currency of the Future or Do the Costs Come Down and It Is Commoditised?\n\n- We are forgetting \u201ccompute\u201d is a euphemism for a $2BN data center in the middle of nowhere.\n- They have GPUs & liquid cooling systems that need to be constantly optimized & upgraded.\n- We cannot simply build 15 years worth of compute all at once. \u200aNo one's ever going to train a frontier model on the same data center twice.\n\n4. Data Centre Teams and Model Teams Need to Be Coupled\n\n- You cannot have separate teams running data centers & building models.\n- Elon & Zuck control all their data centres.\n- It will not work as these models get better & better.\n\n5. Why the Data Center Is the Most Important Asset\n\n- There is not much difference between these models today.\n- AI researchers are jumping from one lab to another.\n- Scaling laws will become more dominant as these models get bigger.\n\n6. Servers Steel and Power: The Three Bottlenecks of AI\n\nThree things:\n1. Servers: NVIDIA, AMD, Broadcom\u2026\n2. Steel: Construction & real estate developers for data centers.\n3. Power: All the energy required for AI.\n- More factories will be built in 12 months for AI and we will see another industrial revolution.\n\n7. You Have to Have a Cash Machine to Play the Big Model Game\n\n- Amazon has AWS\n- Meta has Instagram\n- Microsoft has Azure\n- You need a cash machine to compete and it CANNOT be the AI business.\n\n8. There is One Definition of Success in Venture: Are you a Slugger?\n\n- If you are generating billion dollar gains, you are a slugger.\n- Doug Leone, Pat Grady, Andrew Reed are all sluggers.\n- You cannot be good at this business without being a slugger.\n\n(links in comments)\n\n#founder #funding #business #investing #vc #venturecapital #entrepreneur #startup #seed #funding #sequoia",
        "location": "190\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                26 Comments"
      },
      {
        "link": "https://www.linkedin.com/in/frieaberg?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Friea Berg\n        \n              \n          \ud83c\udfafData was, is, and will be the only fuel. Analogy with the fossil fuel is a good one, since the data we have today is as dirty as oil and coal. What big tech companies tried is to put a lot of \"garbage\" data, and what they did get was fantastic. Yet, we are at the point of diminishing returns. Data is the beginning and the end of this cycle.\u201d\n\nGreat thread, particularly this comment \ud83d\udc46from Ilija Lazarevic",
        "location": "20\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                2 Comments"
      },
      {
        "link": "https://uk.linkedin.com/in/marchollyoak?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Marc Hollyoak\n        \n              \n          Matteo Sorci's and Oliver Molander's posts highlight a sea change in AI. Ilya Sutskever refers to the death of the large language model (LLM) pre-training scaling laws and potentially new ways these models can be employed. This is an occasion for more intelligent implementation and embedding into business processes without scaling models.\n\nInterestingly, the AI world has already hit a wall regarding pre-training and data availability. Future AI development should be more agent-oriented and tool-related, just like how an increased sophistication developed in the human brain but without the increase in size.\n\nThis marks a paradigm shift of AI toward an application and integration strategy instead of a scaling-up practice. AI would then be aimed at more practicable and relevant applications across different industries.",
        "location": "1\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                2 Comments"
      },
      {
        "link": "https://uk.linkedin.com/in/shahid-manzoor-bhat-524b4a34?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Shahid Manzoor Bhat\n        \n              \n          Ilya Sutskever at NeurIPS confirms: pre-training scaling for LLMs has stalled. Compute scales, but data doesn\u2019t\u2014neither new nor synthetic data is moving the needle. Alexandr Wang from Scale AI agrees: \"The data wall is real, and synthetic data isn\u2019t delivering.\"\n\nThe next breakthroughs might come from focusing less on scaling and more on orchestration\u2014harnessing multiple models and tools to solve complex, real-world problems collaboratively.\n\nWhat are your thoughts\n\n#ArtificialIntelligence #FutureOfAI #AITrends #LLMs",
        "location": "5\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                1 Comment"
      },
      {
        "link": "https://www.linkedin.com/in/alireza-seif-1baaa9225?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Alireza Seif\n        \n              \n          We recently posted a new preprint \"Entanglement-enhanced learning of quantum processes at scale\" (https://lnkd.in/eCeFCrF5) with Senrui Chen, Swarnadeep Majumder, Haoran Liao, Derek S. Wang, Moein Malekakhlagh, Ali Javadi, Liang Jiang, and Zlatko Minev. \n\nCheck out the nice summary by Senrui below (https://lnkd.in/eWBkCAGS):\n\n\"Can (noisy) quantum entanglement provide advantages in learning physical processes of practical interest?  \n\nWe give a positive answer to this question, in the task of learning the Pauli noise processes on a quantum device.\n\nPauli noise learning is a well-studied task in the literature of quantum noise characterization. Recent works have proven rigorous efficiency enhancement in Pauli channel learning using entanglement with quantum memory, but it remains unclear whether such enhancement can survive noise and be practically useful.  \n\nInspired by techniques from quantum benchmarking and error mitigation, we develop an entanglement-enhanced Pauli channel learning scheme that is both noise-resilient and provably efficient. Experimental results with IBM Quantum confirm our protocol yield consistent estimates with standard entanglement-free learning schemes, while possesses a significant improvement in sample efficiency, even on current noisy quantum hardware.  \n\nOur work showcases quantum entanglement as a resource are already giving us enhancement on current quantum devices. Our protocol also opens up new possibility for quantum noise characterization at scale.\"",
        "location": "105\n              \n            \n      \n  \n                  \n                  \n                  \n                  \n                  \n                  \n                  \n            \n      \n        \n                3 Comments"
      },
      {
        "link": "https://fr.linkedin.com/in/barbaresco?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Fr\u00e9d\u00e9ric Barbaresco\n        \n              \n          Lie Group Decompositions for Equivariant Neural Networks\nhttps://lnkd.in/edWHiHPe\nMircea Mironenco,\u00a0Patrick Forr\u00e9\nAbstract:\nInvariance and equivariance to geometrical transformations have proven to be very useful inductive biases when training (convolutional) neural network models, especially in the low-data regime. Much work has focused on the case where the symmetry group employed is compact or abelian, or both. Recent work has explored enlarging the class of transformations used to the case of Lie groups, principally through the use of their Lie algebra, as well as the group exponential and logarithm maps. The applicability of such methods to larger transformation groups is limited by the fact that depending on the group of interest\u00a0\ufffd, the exponential map may not be surjective. Further limitations are encountered when\u00a0\ufffd is neither compact nor abelian. Using the structure and geometry of Lie groups and their homogeneous spaces, we present a framework by which it is possible to work with such groups primarily focusing on the Lie groups\u00a0\ufffd=GL+(\ufffd,\ufffd) and\u00a0\ufffd=SL(\ufffd,\ufffd) , as well as their representation as affine transformations\u00a0\ufffd\ufffd\u22ca\ufffd. Invariant integration as well as a global parametrization is realized by decomposing the \"larger\" groups into subgroups and submanifolds which can be handled individually. Under this framework, we show how convolution kernels can be parametrized to build models equivariant with respect to affine transformations. We evaluate the robustness and out-of-distribution generalisation capability of our model on the standard affine-invariant benchmark classification task, where we outperform all previous equivariant models as well as all Capsule Network proposals.",
        "location": "10"
      },
      {
        "link": "https://in.linkedin.com/in/nishant-sinha-a610311?trk=public_profile_relatedPosts_face-pile-cta",
        "name": "",
        "summary": "Nishant Sinha\n        \n              \n          Karpathy recalls Yann's \"cake\" after almost a decade. It is amazing how top researchers (and their intuitions) fall prey to well-hyped \"local minima\" of ideas, over and over again. and then there are some who don't.\n\nWhile he downplays RLHF to be the petite cherry on the cake, the dust hasn't settled on that yet. In spite of DPO, SPO, and what not, researchers keep writing paper saying that the cherry is the crowning glory of the cake.\n\n---\nGet the latest AI news and updates at offnote.substack.com.",
        "location": "45"
      }
    ],
    "similar_profiles": [],
    "recommendations": [],
    "publications": [
      {
        "name": "Annual field-scale maps of tall and short crops at the global scale using GEDI and Sentinel-2",
        "sub_title": "",
        "summary": "",
        "date": "",
        "link": "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Fwww%2Emdpi%2Ecom%2F2072-4292%2F15%2F17%2F4123&urlhash=MGLT&trk=public_profile_publication-title"
      },
      {
        "name": "CriticAL: Critic Automation with Language Models",
        "sub_title": "",
        "summary": "",
        "date": "",
        "link": "https://www.linkedin.com/redir/redirect?url=https%3A%2F%2Farxiv%2Eorg%2Fabs%2F2411%2E06590&urlhash=nA0q&trk=public_profile_publication-title"
      }
    ],
    "courses": [],
    "languages": [],
    "organizations": [],
    "projects": [],
    "awards": [
      {
        "name": "Cal Hacks 10.0 Best Web3 Hack",
        "organization": "-",
        "duration": "Oct 2023",
        "summary": "Built WorldDex: WorldDex is a real-life Pok\u00e9dex: on our mobile app, you can scan and capture any object, talk to your collection, and share your experiences (using Computer Vision, LLMs, Web3, and more)!"
      },
      {
        "name": "PennApps XXIV 1st Place",
        "organization": "-",
        "duration": "Sep 2023",
        "summary": "Built CharactAR: An interactive, generative AI + AR experience with personalized dialogue, dynamically generated 3D objects, and a custom character builder."
      },
      {
        "name": "International Earth Science Olympiad Silver Medal",
        "organization": "-",
        "duration": "Aug 2019",
        "summary": ""
      }
    ],
    "score": []
  }
]