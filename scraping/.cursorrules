# Instructions

You are a multi-agent system coordinator, playing two roles in this environment: Planner and Executor. You will decide the next steps based on the current state of `Multi-Agent Scratchpad` section in the `.cursorrules` file. Your goal is to complete the user's (or business's) final requirements. The specific instructions are as follows:

## Role Descriptions

1. Planner

    * Responsibilities: Perform high-level analysis, break down tasks, define success criteria, evaluate current progress. When doing planning, always use high-intelligence models (OpenAI o1 via `tools/plan_exec_llm.py`). Don't rely on your own capabilities to do the planning.
    * Actions: Invoke the Planner by calling `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt}`. You can also include content from a specific file in the analysis by using the `--file` option: `.venv/bin/python tools/plan_exec_llm.py --prompt {any prompt} --file {path/to/file}`. It will print out a plan on how to revise the `.cursorrules` file. You then need to actually do the changes to the file. And then reread the file to see what's the next step.

2) Executor

    * Responsibilities: Execute specific tasks instructed by the Planner, such as writing code, running tests, handling implementation details, etc.. The key is you need to report progress or raise questions to the Planner at the right time, e.g. after completion some milestone or after you've hit a blocker.
    * Actions: When you complete a subtask or need assistance/more information, also make incremental writes or modifications to the `Multi-Agent Scratchpad` section in the `.cursorrules` file; update the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections. And then change to the Planner role.

## Document Conventions

* The `Multi-Agent Scratchpad` section in the `.cursorrules` file is divided into several sections as per the above structure. Please do not arbitrarily change the titles to avoid affecting subsequent reading.
* Sections like "Background and Motivation" and "Key Challenges and Analysis" are generally established by the Planner initially and gradually appended during task progress.
* "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" are mainly filled by the Executor, with the Planner reviewing and supplementing as needed.
* "Next Steps and Action Items" mainly contains specific execution steps written by the Planner for the Executor.

## Workflow Guidelines

* After you receive an initial prompt for a new task, update the "Background and Motivation" section, and then invoke the Planner to do the planning.
* When thinking as a Planner, always use the local command line `python tools/plan_exec_llm.py --prompt {any prompt}` to call the o1 model for deep analysis, recording results in sections like "Key Challenges and Analysis" or "High-level Task Breakdown". Also update the "Background and Motivation" section.
* When you as an Executor receive new instructions, use the existing cursor tools and workflow to execute those tasks. After completion, write back to the "Current Status / Progress Tracking" and "Executor's Feedback or Assistance Requests" sections in the `Multi-Agent Scratchpad`.
* If unclear whether Planner or Executor is speaking, declare your current role in the output prompt.
* Continue the cycle unless the Planner explicitly indicates the entire project is complete or stopped. Communication between Planner and Executor is conducted through writing to or modifying the `Multi-Agent Scratchpad` section.

Please note:

* Note the task completion should only be announced by the Planner, not the Executor. If the Executor thinks the task is done, it should ask the Planner for confirmation. Then the Planner needs to do some cross-checking.
* Avoid rewriting the entire document unless necessary;
* Avoid deleting records left by other roles; you can append new paragraphs or mark old paragraphs as outdated;
* When new external information is needed, you can use command line tools (like search_engine.py, llm_api.py), but document the purpose and results of such requests;
* Before executing any large-scale changes or critical functionality, the Executor should first notify the Planner in "Executor's Feedback or Assistance Requests" to ensure everyone understands the consequences.
* During you interaction with the user, if you find anything reusable in this project (e.g. version of a library, model name), especially about a fix to a mistake you made or a correction you received, you should take note in the `Lessons` section in the `.cursorrules` file so you will not make the same mistake again. 

# Tools

Note all the tools are in python. So in the case you need to do batch processing, you can always consult the python files and write your own script.

## Screenshot Verification
The screenshot verification workflow allows you to capture screenshots of web pages and verify their appearance using LLMs. The following tools are available:

1. Screenshot Capture:
```bash
.venv/bin/python tools/screenshot_utils.py URL [--output OUTPUT] [--width WIDTH] [--height HEIGHT]
```

2. LLM Verification with Images:
```bash
.venv/bin/python tools/llm_api.py --prompt "Your verification question" --provider {openai|anthropic} --image path/to/screenshot.png
```

Example workflow:
```python
from screenshot_utils import take_screenshot_sync
from llm_api import query_llm

# Take a screenshot
screenshot_path = take_screenshot_sync('https://example.com', 'screenshot.png')

# Verify with LLM
response = query_llm(
    "What is the background color and title of this webpage?",
    provider="openai",  # or "anthropic"
    image_path=screenshot_path
)
print(response)
```

## LLM

You always have an LLM at your side to help you with the task. For simple tasks, you could invoke the LLM by running the following command:
```
.venv/bin/python ./tools/llm_api.py --prompt "What is the capital of France?" --provider "anthropic"
```

The LLM API supports multiple providers:
- OpenAI (default, model: gpt-4o)
- Azure OpenAI (model: configured via AZURE_OPENAI_MODEL_DEPLOYMENT in .env file, defaults to gpt-4o-ms)
- DeepSeek (model: deepseek-chat)
- Anthropic (model: claude-3-sonnet-20240229)
- Gemini (model: gemini-pro)
- Local LLM (model: Qwen/Qwen2.5-32B-Instruct-AWQ)

But usually it's a better idea to check the content of the file and use the APIs in the `tools/llm_api.py` file to invoke the LLM if needed.

## Web browser

You could use the `tools/web_scraper.py` file to scrape the web.
```
.venv/bin/python ./tools/web_scraper.py --max-concurrent 3 URL1 URL2 URL3
```
This will output the content of the web pages.

## Search engine

You could use the `tools/search_engine.py` file to search the web.
```
.venv/bin/python ./tools/search_engine.py "your search keywords"
```
This will output the search results in the following format:
```
URL: https://example.com
Title: This is the title of the search result
Snippet: This is a snippet of the search result
```
If needed, you can further use the `web_scraper.py` file to scrape the web page content.

# Lessons

## User Specified Lessons

- You have a uv python venv in ./.venv. Always use it when running python scripts. It's a uv venv, so use `uv pip install` to install packages. And you need to activate it first. When you see errors like `no such file or directory: .venv/bin/uv`, that means you didn't activate the venv.
- Include info useful for debugging in the program output.
- Read the file before you try to edit it.
- Due to Cursor's limit, when you use `git` and `gh` and need to submit a multiline commit message, first write the message in a file, and then use `git commit -F <filename>` or similar command to commit. And then remove the file. Include "[Cursor] " in the commit message and PR title.

## Cursor learned

- For search results, ensure proper handling of different character encodings (UTF-8) for international queries
- Add debug information to stderr while keeping the main output clean in stdout for better pipeline integration
- When using seaborn styles in matplotlib, use 'seaborn-v0_8' instead of 'seaborn' as the style name due to recent seaborn version changes
- Use `gpt-4o` as the model name for OpenAI. It is the latest GPT model and has vision capabilities as well. `o1` is the most advanced and expensive model from OpenAI. Use it when you need to do reasoning, planning, or get blocked.
- Use `claude-3-5-sonnet-20241022` as the model name for Claude. It is the latest Claude model and has vision capabilities as well.
- Use Together AI as the provider for DeepSeek and other models:
  * DeepSeek Chat: `deepseek-ai/deepseek-chat`
  * LLaMA 2 70B: `togethercomputer/llama-2-70b-chat`
  * Mixtral 8x7B: `mistralai/Mixtral-8x7B-Instruct-v0.1`
  * Qwen 72B: `Qwen/Qwen-72B-Chat`

# Multi-Agent Scratchpad

## Background and Motivation

The goal remains to create a networking tool that accepts a list of names and emails, performs searches for URLs related to those names, collects HTML content, and saves that content for later parsing and analysis. The project strategy should emphasize quick prototyping (to validate feasibility) and iterative refinements (to address challenges early on). Leveraging small proof-of-concept steps helps de-risk potential issues in web scraping, rate limiting, or inefficient data handling.

Key Architectural Components & Challenges:
1. Input Handling Layer:
   - Reads and validates name and email inputs (e.g., CSV file).
   - Basic cleansing of names/emails (e.g., trimming whitespace).

2. Web Search Layer:
   - Integrates with a simple search API (e.g., Bing, Google, or a free alternative).
   - Monitors quota usage and rate limits to avoid blocking.
   - Scalable design (batch search queries).

3. Web Scraper & Parser Layer:
   - Fetches HTML content from discovered URLs.
   - Uses robust error handling to skip invalid or unreachable URLs.
   - Employs lightweight parsing libraries (like Beautiful Soup) to extract text content.
   - Captures metadata (title, snippet) for faster indexing.

4. Data Storage Layer:
   - Persists raw HTML files and parsed text in an organized folder structure or database.
   - Implements versioning or timestamped folders for revisits or updates.
   - Ensures data can be easily retrieved for future analysis.

5. Iterative Development and Probing Experiments:
   - Start with a small subset of names and emails to validate end-to-end flow.
   - Add logging to track search queries, scraping success, and total results.
   - Conduct performance tests on a limited set of queries.

6. Extension and Interface:
   - A simple CLI for initial testing (supports specifying input CSV).
   - Optional web-based GUI for more user-friendly interactions.
   - Automated testing for each layer (mocks for web search and scraper).

## Key Challenges and Analysis

1. Web Search Challenges:
   - Rate limiting: Most search engines limit the number of requests per time period
   - Search query construction: Creating effective queries from names and emails
   - Result relevance: Ensuring the URLs are actually related to the target person
   - API costs: Some search APIs may have usage costs

2. Web Scraping Challenges:
   - Dynamic content: Some websites use JavaScript to load content
   - Anti-scraping measures: Websites may block automated scraping
   - Content structure variation: Different websites structure their content differently
   - Error handling: Dealing with timeouts, connection errors, and invalid HTML

3. Data Processing Challenges:
   - Content extraction: Identifying relevant information from HTML
   - Data organization: Structuring the saved data for easy retrieval
   - Storage efficiency: Managing potentially large amounts of data
   - Metadata tracking: Keeping track of search terms, sources, and timestamps

4. Technical Implementation Challenges:
   - Concurrency: Balancing parallel processing with rate limits
   - Error recovery: Resuming from failures without losing progress
   - Performance optimization: Ensuring the tool runs efficiently
   - Cross-platform compatibility: Ensuring the tool works across different operating systems

## Verifiable Success Criteria

1. Functionality:
   - The tool successfully processes a list of names and emails
   - It performs web searches and retrieves relevant URLs
   - It scrapes and parses HTML content from those URLs
   - It saves the data in an organized, retrievable format

2. Performance:
   - The tool handles at least 100 name-email pairs efficiently
   - Search and scraping operations respect rate limits
   - Processing time is reasonable (benchmark: <30 seconds per person)

3. Usability:
   - Clear command-line interface with help documentation
   - Informative progress reporting during execution
   - Organized output structure that's easy to navigate
   - Error messages that provide actionable information

4. Robustness:
   - Graceful handling of network errors and timeouts
   - Recovery from partial failures without losing progress
   - Validation of inputs and outputs at each stage
   - Logging of operations for debugging and auditing

## High-level Task Breakdown

1. Setup Phase:
   - Create project structure and initialize repository
   - Set up virtual environment and install dependencies
   - Create configuration file template

2. Input Handling Implementation:
   - Develop CSV/JSON parser for name-email pairs
   - Implement input validation and cleansing
   - Create search query generator from personal data

3. Search Implementation:
   - Integrate with search_engine.py
   - Implement rate limiting and quota management
   - Develop URL collector and filter for relevance

4. Web Scraping Implementation:
   - Integrate with web_scraper.py
   - Enhance error handling for various failure modes
   - Implement concurrent scraping with rate limiting

5. Data Storage Implementation:
   - Design folder structure for organized storage
   - Implement file naming conventions and metadata
   - Create indexing system for easy retrieval

6. Integration and Testing:
   - Connect all components into a unified workflow
   - Develop end-to-end tests with sample data
   - Perform performance and stress testing

7. Documentation and Finalization:
   - Write comprehensive README and usage guide
   - Document code with docstrings and comments
   - Create example scripts for common use cases

## Current Status / Progress Tracking

Implementation of the networking tool is complete. The following components have been created:

1. Project Structure:
   - Created `network_tool` directory with `core` and `utils` subdirectories
   - Created `data` directory with `input` and `output` subdirectories
   - Created `tests` directory for unit tests

2. Input Handling Module:
   - Implemented `InputHandler` class in `network_tool/utils/input_handler.py`
   - Added support for CSV and JSON input formats
   - Implemented input validation and cleansing
   - Added search query generation functionality

3. Search Module:
   - Implemented `SearchHandler` class in `network_tool/utils/search_handler.py`
   - Integrated with `search_engine.py` tool
   - Added rate limiting and error handling
   - Implemented result filtering for relevance

4. Web Scraping Module:
   - Implemented `ScraperHandler` class in `network_tool/utils/scraper_handler.py`
   - Integrated with `web_scraper.py` tool
   - Added concurrent scraping with rate limiting
   - Implemented file saving functionality

5. Core Module:
   - Implemented `NetworkTool` class in `network_tool/core/network_tool.py`
   - Integrated all components into a complete workflow
   - Added statistics tracking and reporting
   - Implemented error handling and logging

6. Command-line Interface:
   - Created `__main__.py` for command-line usage
   - Added command-line argument parsing
   - Implemented user-friendly output formatting

7. Documentation and Testing:
   - Created comprehensive README with usage instructions
   - Added sample input files for testing
   - Created unit tests for the input handler
   - Updated requirements.txt with dependencies

The tool is now ready for testing with real data.

## Next Steps and Action Items

1. Expand error handling and monitoring:
   - Add retry logic for HTTP 403 errors (e.g., by rotating user agents or introducing backoff).
   - Improve handling of non-UTF-8 content, including PDFs or other file types.
   - Enhance logging around failed or partially successful scrapes for simpler debugging.
2. Refine and optimize search flow:
   - Add caching of already-searched names/emails to avoid repeated requests.
   - Experiment with additional query parameters (e.g., synonyms, wildcard, partial matches) to improve relevance.
   - Consider offering a fallback search engine if one service's requests exceed quota or trigger rate limits.
3. Enhance performance through targeted concurrency:
   - Test with slightly larger sets of people (e.g., 10–20) to ensure concurrency does not cause bottlenecks.
   - Use more flexible concurrency controls to dynamically adjust parallel scraping based on response times or API quotas.
4. Consider headless browser integration for dynamic sites:
   - Use a headless browser (e.g., Selenium, Playwright) in cases where JavaScript loading is critical for scraping.
   - Evaluate the additional resource cost to ensure it is justified by improved coverage.
5. Continue incremental testing and feedback loops:
   - Run regular end-to-end tests before and after each improvement.
   - Keep track of run times, error rates, and memory usage to detect regressions early.
   - Collect user feedback and documentation updates as new features are added.

## Executor's Feedback or Assistance Requests

I've completed the small, targeted load tests with both CSV and JSON sample files as instructed. Here are my findings:

1. Performance and Rate Limiting:
   - The tool successfully processed 5 people in 24.32 seconds (CSV) and 17.31 seconds (JSON).
   - The rate limiting worked as expected, with appropriate delays between searches and scraping tasks.
   - The concurrency control for scraping worked well, limiting to 2 concurrent tasks as specified.

2. Search Results and Filtering:
   - The tool found 11 URLs (CSV) and 10 URLs (JSON) across all 5 people.
   - The filtering mechanism correctly excluded social media sites like LinkedIn.
   - For some people (John Smith), no relevant results were found after filtering.

3. Web Scraping and Error Handling:
   - The tool successfully scraped 9 URLs in both tests.
   - Error handling worked as expected, with appropriate error messages for failed scrapes.
   - Two types of errors were encountered:
     - HTTP 403 (Forbidden) for Emily Wilson's URL
     - Encoding error for a PDF file in John Smith's results

4. Data Storage:
   - The tool created well-organized directories for each person.
   - Each URL's data was saved in three files: HTML, text, and metadata.
   - Search results were saved in JSON format for reference.
   - Statistics were saved in a JSON file with useful metrics.

5. Potential Improvements:
   - Add support for non-UTF-8 encoded content (like PDFs)
   - Implement retry logic for HTTP 403 errors with different user agents
   - Add caching to avoid repeated searches for the same person
   - Enhance the search query generation for better results
   - Consider using a headless browser for JavaScript-heavy websites

The tool is working as expected for the basic use case, but there are opportunities for improvement in handling edge cases and optimizing performance.

I've now implemented the first two improvements from the list:

1. Retry Logic with Rotating User Agents:
   - Added a list of common user agents to the `ScraperHandler` class
   - Implemented a `_fetch_with_retry` method that attempts to fetch a URL multiple times using different user agents
   - Added exponential backoff logic for retry attempts
   - Updated the `NetworkTool` class to accept `max_retries` and `retry_delay` parameters
   - Modified the command-line interface to include these new parameters

2. Support for Non-UTF-8 Content:
   - Added a `_handle_non_utf8_content` method that tries different encodings (latin-1, iso-8859-1, windows-1252)
   - Implemented fallback logic to extract printable characters from raw bytes if all encodings fail
   - Added a `_handle_pdf_content` method that extracts text from PDF files using PyPDF2
   - Updated the requirements.txt file to include PyPDF2
   - Updated the README.md file to document these new features

These improvements should address the two main error types encountered during the load tests:
- HTTP 403 errors will now be retried with different user agents
- Encoding errors and PDF files will now be handled properly

The next steps could include:
1. Implementing caching to avoid repeated searches for the same person
2. Enhancing the search query generation for better results
3. Adding support for JavaScript-heavy websites using a headless browser

Would you like me to implement any of these additional improvements?